{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10e650e",
   "metadata": {},
   "source": [
    "CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca706df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"/home/tommy/Project/PcodeBERT/dataset/csv/base_dataset_filtered.csv\"\n",
    "\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "#only save family is benign, mirai, gafgyt, tsunami\n",
    "df = df[df[\"family\"].isin([\"benign\", \"mirai\", \"gafgyt\", \"tsunami\"])]\n",
    "df = df.reset_index(drop=True)\n",
    "df.to_csv(\"/home/tommy/Project/PcodeBERT/dataset/csv/base_dataset_filtered_v2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564c3e40",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eee657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Regex pattern preprocessing\n",
    "#1)  opcode_pattern: Extract P-Code\n",
    "#2)  opcode_pattern: Extract Calculation\n",
    "OPCODE_PAT = re.compile(r\"(?:\\)\\s+|---\\s+)([A-Z_]+)\")\n",
    "OPERAND_PAT = re.compile(r\"\\(([^ ,]+)\\s*,\\s*[^,]*,\\s*([0-9]+)\\)\")\n",
    "\n",
    "def _map_operand(op_type: str) -> str:\n",
    "    op_type_l = op_type.lower()\n",
    "    if op_type_l == 'register':\n",
    "        return \"REG\"\n",
    "    if op_type_l == 'ram':\n",
    "        return \"MEM\"\n",
    "    if op_type_l in {'const', 'constant'}:\n",
    "        return \"CONST\"\n",
    "    if op_type_l == 'unique':\n",
    "        return \"UNIQUE\"\n",
    "    if op_type_l == 'stack':\n",
    "        return \"STACK\"\n",
    "    return \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e9f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch \n",
    "from transformers import AutoTokenizer, RobertaForMaskedLM \n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def _tokens_from_operation(operation_str: str) -> list[str]:\n",
    "    if not operation_str:\n",
    "        return []\n",
    "    m = OPCODE_PAT.search(operation_str)\n",
    "    if not m:\n",
    "        return []\n",
    "    opcode = m.group(1)\n",
    "    tokens = [opcode]\n",
    "    operands = OPERAND_PAT.findall(operation_str)\n",
    "    for op_type, _ in operands:\n",
    "        tokens.append(_map_operand(op_type))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def load_data_from_folder(folder_path, archs):\n",
    "    arch_data = {arch: {} for arch in archs}\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        subfolder = os.path.basename(root)\n",
    "        arch = next((a for a in archs if a in subfolder), None)\n",
    "        if not arch:\n",
    "            continue\n",
    "\n",
    "        file_base = subfolder.split(\"_\")[-1]\n",
    "        for fname in files:\n",
    "            if not fname.endswith(\".json\"):\n",
    "                continue\n",
    "            path = os.path.join(root, fname)\n",
    "            try:\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                for _, func in data.items():\n",
    "                    fn = func.get(\"function_name\", \"\").strip()\n",
    "                    instrs = func.get(\"instructions\", [])\n",
    "                    if not fn or not instrs:\n",
    "                        continue\n",
    "\n",
    "                    flat_tokens: list[str] = []\n",
    "                    for ins in instrs:\n",
    "                        op = ins.get(\"operation\", \"\").strip()\n",
    "                        if not op:\n",
    "                            continue\n",
    "                        sent = _tokens_from_operation(op)\n",
    "                        if sent:\n",
    "                            flat_tokens.extend(sent)\n",
    "\n",
    "                    if not flat_tokens:\n",
    "                        continue\n",
    "\n",
    "                    tokenized_line = \" \".join(flat_tokens)\n",
    "                    key = f\"{file_base}::{fn}\"\n",
    "                    arch_data[arch][key] = (file_base, fn, tokenized_line)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"{file_base} 讀取失敗，跳過: {e}\")\n",
    "    return arch_data\n",
    "\n",
    "\n",
    "def load_pretrained_model():\n",
    "    \"\"\"載入預訓練的模型和tokenizer\"\"\"\n",
    "    model_path = \"/home/tommy/Project/PcodeBERT/outputs/models/pretrain\"\n",
    "    \n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = RobertaForMaskedLM.from_pretrained(model_path)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded successfully on device: {device}\")\n",
    "    return model, tokenizer, device\n",
    "\n",
    "def get_sentence_embedding(sentence, model, tokenizer, device):\n",
    "    \"\"\"對單個sentence生成embedding\"\"\"\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # model.roberta(**inputs) -> model(**inputs) for base model\n",
    "        outputs = model(**inputs, output_hidden_states=True) \n",
    "        # 使用最後一層 hidden state 的 [CLS] token embedding\n",
    "        embedding = outputs.hidden_states[-1][:, 0, :].cpu().numpy()\n",
    "    \n",
    "    return embedding[0]\n",
    "\n",
    "\n",
    "def extract_and_vectorize_with_bert():\n",
    "    input_folders = [\n",
    "        \"/home/tommy/Project/PcodeBERT/outputs/align_sentences\"\n",
    "    ]\n",
    "\n",
    "    output_path = \"/home/tommy/Project/PcodeBERT/outputs/alignment_vector\"\n",
    "    archs = [\"mips_32\", \"arm_32\", \"x86_64\"]\n",
    "    print(\"Loading BERT model...\")\n",
    "    model, tokenizer, device = load_pretrained_model()\n",
    "\n",
    "    # 每個來源資料夾都跑一次\n",
    "    arch_datasets = []\n",
    "    print(\"Loading data from folders...\")\n",
    "    for folder in input_folders:\n",
    "        arch_datasets.append(load_data_from_folder(folder, archs))\n",
    "\n",
    "    # 找所有來源的交集\n",
    "    print(\"Finding common keys...\")\n",
    "    common_keys = None\n",
    "    for arch_data in arch_datasets:\n",
    "        if common_keys is None:\n",
    "            common_keys = set(arch_data[\"mips_32\"]) & set(arch_data[\"arm_32\"]) & set(arch_data[\"x86_64\"])\n",
    "        else:\n",
    "            common_keys &= set(arch_data[\"mips_32\"]) & set(arch_data[\"arm_32\"]) & set(arch_data[\"x86_64\"])\n",
    "\n",
    "    common_keys = list(common_keys)\n",
    "    random.shuffle(common_keys)\n",
    "\n",
    "    # 平均切給不同來源\n",
    "    n = len(input_folders)\n",
    "    chunk_size = len(common_keys) // n\n",
    "    key_groups = [common_keys[i*chunk_size:(i+1)*chunk_size] for i in range(n)]\n",
    "    \n",
    "\n",
    "    print(\"Generating embeddings...\")\n",
    "    samples = []\n",
    "    for arch_data, keys in zip(arch_datasets, key_groups):\n",
    "        for i, key in enumerate(keys):\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Processing item {i+1}/{len(keys)}...\")\n",
    "            \n",
    "            _, _, x86_op = arch_data[\"x86_64\"][key]\n",
    "            _, _, arm_op = arch_data[\"arm_32\"][key]\n",
    "            \n",
    "            vec_x = get_sentence_embedding(x86_op, model, tokenizer, device)\n",
    "            vec_a = get_sentence_embedding(arm_op, model, tokenizer, device)\n",
    "            \n",
    "            samples.append((x86_op, arm_op, vec_x, vec_a, 1))\n",
    "    \n",
    "    print(\"Saving results...\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    pk_file = os.path.join(output_path, \"train_arm_vector_mix_bert.pickle\")\n",
    "    with open(pk_file, \"wb\") as f:\n",
    "        pickle.dump([(vec_x, vec_a, label) for _, _, vec_x, vec_a, label in samples], f)\n",
    "\n",
    "    csv_file = os.path.join(output_path, \"train_arm_op_mix_bert.csv\")\n",
    "    with open(csv_file, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"x86_op\", \"arm_op\", \"label\"])\n",
    "        for x86_op, arm_op, _, _, label in samples:\n",
    "            writer.writerow([x86_op, arm_op, label])\n",
    "            \n",
    "    \n",
    "    print(f\"\\n已生成 {pk_file} & {csv_file}，共 {len(samples)} 筆樣本\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_and_vectorize_with_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e596e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PcodeBERT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
