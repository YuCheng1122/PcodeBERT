{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10e650e",
   "metadata": {},
   "source": [
    "CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca706df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"/home/tommy/Project/PcodeBERT/dataset/csv/base_dataset_filtered.csv\"\n",
    "\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "#only save family is benign, mirai, gafgyt, tsunami\n",
    "df = df[df[\"family\"].isin([\"benign\", \"mirai\", \"gafgyt\", \"tsunami\"])]\n",
    "df = df.reset_index(drop=True)\n",
    "df.to_csv(\"/home/tommy/Project/PcodeBERT/dataset/csv/base_dataset_filtered_v2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564c3e40",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02eee657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Regex pattern preprocessing\n",
    "#1)  opcode_pattern: Extract P-Code\n",
    "#2)  opcode_pattern: Extract Calculation\n",
    "OPCODE_PAT = re.compile(r\"(?:\\)\\s+|---\\s+)([A-Z_]+)\")\n",
    "OPERAND_PAT = re.compile(r\"\\(([^ ,]+)\\s*,\\s*[^,]*,\\s*([0-9]+)\\)\")\n",
    "\n",
    "def _map_operand(op_type: str) -> str:\n",
    "    op_type_l = op_type.lower()\n",
    "    if op_type_l == 'register':\n",
    "        return \"REG\"\n",
    "    if op_type_l == 'ram':\n",
    "        return \"MEM\"\n",
    "    if op_type_l in {'const', 'constant'}:\n",
    "        return \"CONST\"\n",
    "    if op_type_l == 'unique':\n",
    "        return \"UNIQUE\"\n",
    "    if op_type_l == 'stack':\n",
    "        return \"STACK\"\n",
    "    return \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e9f6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommy/miniconda3/envs/PcodeBERT/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT model...\n",
      "Loading model from: /home/tommy/Project/PcodeBERT/outputs/models/pretrain\n",
      "Model loaded successfully on device: cuda\n",
      "Loading data from folders...\n",
      "libglpk.so.40.3.1 讀取失敗，跳過: Expecting property name enclosed in double quotes: line 1193205 column 4 (char 49979392)\n",
      "Finding common keys...\n",
      "Generating embeddings...\n",
      "Processing item 100/66607...\n",
      "Processing item 200/66607...\n",
      "Processing item 300/66607...\n",
      "Processing item 400/66607...\n",
      "Processing item 500/66607...\n",
      "Processing item 600/66607...\n",
      "Processing item 700/66607...\n",
      "Processing item 800/66607...\n",
      "Processing item 900/66607...\n",
      "Processing item 1000/66607...\n",
      "Processing item 1100/66607...\n",
      "Processing item 1200/66607...\n",
      "Processing item 1300/66607...\n",
      "Processing item 1400/66607...\n",
      "Processing item 1500/66607...\n",
      "Processing item 1600/66607...\n",
      "Processing item 1700/66607...\n",
      "Processing item 1800/66607...\n",
      "Processing item 1900/66607...\n",
      "Processing item 2000/66607...\n",
      "Processing item 2100/66607...\n",
      "Processing item 2200/66607...\n",
      "Processing item 2300/66607...\n",
      "Processing item 2400/66607...\n",
      "Processing item 2500/66607...\n",
      "Processing item 2600/66607...\n",
      "Processing item 2700/66607...\n",
      "Processing item 2800/66607...\n",
      "Processing item 2900/66607...\n",
      "Processing item 3000/66607...\n",
      "Processing item 3100/66607...\n",
      "Processing item 3200/66607...\n",
      "Processing item 3300/66607...\n",
      "Processing item 3400/66607...\n",
      "Processing item 3500/66607...\n",
      "Processing item 3600/66607...\n",
      "Processing item 3700/66607...\n",
      "Processing item 3800/66607...\n",
      "Processing item 3900/66607...\n",
      "Processing item 4000/66607...\n",
      "Processing item 4100/66607...\n",
      "Processing item 4200/66607...\n",
      "Processing item 4300/66607...\n",
      "Processing item 4400/66607...\n",
      "Processing item 4500/66607...\n",
      "Processing item 4600/66607...\n",
      "Processing item 4700/66607...\n",
      "Processing item 4800/66607...\n",
      "Processing item 4900/66607...\n",
      "Processing item 5000/66607...\n",
      "Processing item 5100/66607...\n",
      "Processing item 5200/66607...\n",
      "Processing item 5300/66607...\n",
      "Processing item 5400/66607...\n",
      "Processing item 5500/66607...\n",
      "Processing item 5600/66607...\n",
      "Processing item 5700/66607...\n",
      "Processing item 5800/66607...\n",
      "Processing item 5900/66607...\n",
      "Processing item 6000/66607...\n",
      "Processing item 6100/66607...\n",
      "Processing item 6200/66607...\n",
      "Processing item 6300/66607...\n",
      "Processing item 6400/66607...\n",
      "Processing item 6500/66607...\n",
      "Processing item 6600/66607...\n",
      "Processing item 6700/66607...\n",
      "Processing item 6800/66607...\n",
      "Processing item 6900/66607...\n",
      "Processing item 7000/66607...\n",
      "Processing item 7100/66607...\n",
      "Processing item 7200/66607...\n",
      "Processing item 7300/66607...\n",
      "Processing item 7400/66607...\n",
      "Processing item 7500/66607...\n",
      "Processing item 7600/66607...\n",
      "Processing item 7700/66607...\n",
      "Processing item 7800/66607...\n",
      "Processing item 7900/66607...\n",
      "Processing item 8000/66607...\n",
      "Processing item 8100/66607...\n",
      "Processing item 8200/66607...\n",
      "Processing item 8300/66607...\n",
      "Processing item 8400/66607...\n",
      "Processing item 8500/66607...\n",
      "Processing item 8600/66607...\n",
      "Processing item 8700/66607...\n",
      "Processing item 8800/66607...\n",
      "Processing item 8900/66607...\n",
      "Processing item 9000/66607...\n",
      "Processing item 9100/66607...\n",
      "Processing item 9200/66607...\n",
      "Processing item 9300/66607...\n",
      "Processing item 9400/66607...\n",
      "Processing item 9500/66607...\n",
      "Processing item 9600/66607...\n",
      "Processing item 9700/66607...\n",
      "Processing item 9800/66607...\n",
      "Processing item 9900/66607...\n",
      "Processing item 10000/66607...\n",
      "Processing item 10100/66607...\n",
      "Processing item 10200/66607...\n",
      "Processing item 10300/66607...\n",
      "Processing item 10400/66607...\n",
      "Processing item 10500/66607...\n",
      "Processing item 10600/66607...\n",
      "Processing item 10700/66607...\n",
      "Processing item 10800/66607...\n",
      "Processing item 10900/66607...\n",
      "Processing item 11000/66607...\n",
      "Processing item 11100/66607...\n",
      "Processing item 11200/66607...\n",
      "Processing item 11300/66607...\n",
      "Processing item 11400/66607...\n",
      "Processing item 11500/66607...\n",
      "Processing item 11600/66607...\n",
      "Processing item 11700/66607...\n",
      "Processing item 11800/66607...\n",
      "Processing item 11900/66607...\n",
      "Processing item 12000/66607...\n",
      "Processing item 12100/66607...\n",
      "Processing item 12200/66607...\n",
      "Processing item 12300/66607...\n",
      "Processing item 12400/66607...\n",
      "Processing item 12500/66607...\n",
      "Processing item 12600/66607...\n",
      "Processing item 12700/66607...\n",
      "Processing item 12800/66607...\n",
      "Processing item 12900/66607...\n",
      "Processing item 13000/66607...\n",
      "Processing item 13100/66607...\n",
      "Processing item 13200/66607...\n",
      "Processing item 13300/66607...\n",
      "Processing item 13400/66607...\n",
      "Processing item 13500/66607...\n",
      "Processing item 13600/66607...\n",
      "Processing item 13700/66607...\n",
      "Processing item 13800/66607...\n",
      "Processing item 13900/66607...\n",
      "Processing item 14000/66607...\n",
      "Processing item 14100/66607...\n",
      "Processing item 14200/66607...\n",
      "Processing item 14300/66607...\n",
      "Processing item 14400/66607...\n",
      "Processing item 14500/66607...\n",
      "Processing item 14600/66607...\n",
      "Processing item 14700/66607...\n",
      "Processing item 14800/66607...\n",
      "Processing item 14900/66607...\n",
      "Processing item 15000/66607...\n",
      "Processing item 15100/66607...\n",
      "Processing item 15200/66607...\n",
      "Processing item 15300/66607...\n",
      "Processing item 15400/66607...\n",
      "Processing item 15500/66607...\n",
      "Processing item 15600/66607...\n",
      "Processing item 15700/66607...\n",
      "Processing item 15800/66607...\n",
      "Processing item 15900/66607...\n",
      "Processing item 16000/66607...\n",
      "Processing item 16100/66607...\n",
      "Processing item 16200/66607...\n",
      "Processing item 16300/66607...\n",
      "Processing item 16400/66607...\n",
      "Processing item 16500/66607...\n",
      "Processing item 16600/66607...\n",
      "Processing item 16700/66607...\n",
      "Processing item 16800/66607...\n",
      "Processing item 16900/66607...\n",
      "Processing item 17000/66607...\n",
      "Processing item 17100/66607...\n",
      "Processing item 17200/66607...\n",
      "Processing item 17300/66607...\n",
      "Processing item 17400/66607...\n",
      "Processing item 17500/66607...\n",
      "Processing item 17600/66607...\n",
      "Processing item 17700/66607...\n",
      "Processing item 17800/66607...\n",
      "Processing item 17900/66607...\n",
      "Processing item 18000/66607...\n",
      "Processing item 18100/66607...\n",
      "Processing item 18200/66607...\n",
      "Processing item 18300/66607...\n",
      "Processing item 18400/66607...\n",
      "Processing item 18500/66607...\n",
      "Processing item 18600/66607...\n",
      "Processing item 18700/66607...\n",
      "Processing item 18800/66607...\n",
      "Processing item 18900/66607...\n",
      "Processing item 19000/66607...\n",
      "Processing item 19100/66607...\n",
      "Processing item 19200/66607...\n",
      "Processing item 19300/66607...\n",
      "Processing item 19400/66607...\n",
      "Processing item 19500/66607...\n",
      "Processing item 19600/66607...\n",
      "Processing item 19700/66607...\n",
      "Processing item 19800/66607...\n",
      "Processing item 19900/66607...\n",
      "Processing item 20000/66607...\n",
      "Processing item 20100/66607...\n",
      "Processing item 20200/66607...\n",
      "Processing item 20300/66607...\n",
      "Processing item 20400/66607...\n",
      "Processing item 20500/66607...\n",
      "Processing item 20600/66607...\n",
      "Processing item 20700/66607...\n",
      "Processing item 20800/66607...\n",
      "Processing item 20900/66607...\n",
      "Processing item 21000/66607...\n",
      "Processing item 21100/66607...\n",
      "Processing item 21200/66607...\n",
      "Processing item 21300/66607...\n",
      "Processing item 21400/66607...\n",
      "Processing item 21500/66607...\n",
      "Processing item 21600/66607...\n",
      "Processing item 21700/66607...\n",
      "Processing item 21800/66607...\n",
      "Processing item 21900/66607...\n",
      "Processing item 22000/66607...\n",
      "Processing item 22100/66607...\n",
      "Processing item 22200/66607...\n",
      "Processing item 22300/66607...\n",
      "Processing item 22400/66607...\n",
      "Processing item 22500/66607...\n",
      "Processing item 22600/66607...\n",
      "Processing item 22700/66607...\n",
      "Processing item 22800/66607...\n",
      "Processing item 22900/66607...\n",
      "Processing item 23000/66607...\n",
      "Processing item 23100/66607...\n",
      "Processing item 23200/66607...\n",
      "Processing item 23300/66607...\n",
      "Processing item 23400/66607...\n",
      "Processing item 23500/66607...\n",
      "Processing item 23600/66607...\n",
      "Processing item 23700/66607...\n",
      "Processing item 23800/66607...\n",
      "Processing item 23900/66607...\n",
      "Processing item 24000/66607...\n",
      "Processing item 24100/66607...\n",
      "Processing item 24200/66607...\n",
      "Processing item 24300/66607...\n",
      "Processing item 24400/66607...\n",
      "Processing item 24500/66607...\n",
      "Processing item 24600/66607...\n",
      "Processing item 24700/66607...\n",
      "Processing item 24800/66607...\n",
      "Processing item 24900/66607...\n",
      "Processing item 25000/66607...\n",
      "Processing item 25100/66607...\n",
      "Processing item 25200/66607...\n",
      "Processing item 25300/66607...\n",
      "Processing item 25400/66607...\n",
      "Processing item 25500/66607...\n",
      "Processing item 25600/66607...\n",
      "Processing item 25700/66607...\n",
      "Processing item 25800/66607...\n",
      "Processing item 25900/66607...\n",
      "Processing item 26000/66607...\n",
      "Processing item 26100/66607...\n",
      "Processing item 26200/66607...\n",
      "Processing item 26300/66607...\n",
      "Processing item 26400/66607...\n",
      "Processing item 26500/66607...\n",
      "Processing item 26600/66607...\n",
      "Processing item 26700/66607...\n",
      "Processing item 26800/66607...\n",
      "Processing item 26900/66607...\n",
      "Processing item 27000/66607...\n",
      "Processing item 27100/66607...\n",
      "Processing item 27200/66607...\n",
      "Processing item 27300/66607...\n",
      "Processing item 27400/66607...\n",
      "Processing item 27500/66607...\n",
      "Processing item 27600/66607...\n",
      "Processing item 27700/66607...\n",
      "Processing item 27800/66607...\n",
      "Processing item 27900/66607...\n",
      "Processing item 28000/66607...\n",
      "Processing item 28100/66607...\n",
      "Processing item 28200/66607...\n",
      "Processing item 28300/66607...\n",
      "Processing item 28400/66607...\n",
      "Processing item 28500/66607...\n",
      "Processing item 28600/66607...\n",
      "Processing item 28700/66607...\n",
      "Processing item 28800/66607...\n",
      "Processing item 28900/66607...\n",
      "Processing item 29000/66607...\n",
      "Processing item 29100/66607...\n",
      "Processing item 29200/66607...\n",
      "Processing item 29300/66607...\n",
      "Processing item 29400/66607...\n",
      "Processing item 29500/66607...\n",
      "Processing item 29600/66607...\n",
      "Processing item 29700/66607...\n",
      "Processing item 29800/66607...\n",
      "Processing item 29900/66607...\n",
      "Processing item 30000/66607...\n",
      "Processing item 30100/66607...\n",
      "Processing item 30200/66607...\n",
      "Processing item 30300/66607...\n",
      "Processing item 30400/66607...\n",
      "Processing item 30500/66607...\n",
      "Processing item 30600/66607...\n",
      "Processing item 30700/66607...\n",
      "Processing item 30800/66607...\n",
      "Processing item 30900/66607...\n",
      "Processing item 31000/66607...\n",
      "Processing item 31100/66607...\n",
      "Processing item 31200/66607...\n",
      "Processing item 31300/66607...\n",
      "Processing item 31400/66607...\n",
      "Processing item 31500/66607...\n",
      "Processing item 31600/66607...\n",
      "Processing item 31700/66607...\n",
      "Processing item 31800/66607...\n",
      "Processing item 31900/66607...\n",
      "Processing item 32000/66607...\n",
      "Processing item 32100/66607...\n",
      "Processing item 32200/66607...\n",
      "Processing item 32300/66607...\n",
      "Processing item 32400/66607...\n",
      "Processing item 32500/66607...\n",
      "Processing item 32600/66607...\n",
      "Processing item 32700/66607...\n",
      "Processing item 32800/66607...\n",
      "Processing item 32900/66607...\n",
      "Processing item 33000/66607...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch \n",
    "from transformers import AutoTokenizer, RobertaForMaskedLM \n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def _tokens_from_operation(operation_str: str) -> list[str]:\n",
    "    if not operation_str:\n",
    "        return []\n",
    "    m = OPCODE_PAT.search(operation_str)\n",
    "    if not m:\n",
    "        return []\n",
    "    opcode = m.group(1)\n",
    "    tokens = [opcode]\n",
    "    operands = OPERAND_PAT.findall(operation_str)\n",
    "    for op_type, _ in operands:\n",
    "        tokens.append(_map_operand(op_type))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def load_data_from_folder(folder_path, archs):\n",
    "    arch_data = {arch: {} for arch in archs}\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        subfolder = os.path.basename(root)\n",
    "        arch = next((a for a in archs if a in subfolder), None)\n",
    "        if not arch:\n",
    "            continue\n",
    "\n",
    "        file_base = subfolder.split(\"_\")[-1]\n",
    "        for fname in files:\n",
    "            if not fname.endswith(\".json\"):\n",
    "                continue\n",
    "            path = os.path.join(root, fname)\n",
    "            try:\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                for _, func in data.items():\n",
    "                    fn = func.get(\"function_name\", \"\").strip()\n",
    "                    instrs = func.get(\"instructions\", [])\n",
    "                    if not fn or not instrs:\n",
    "                        continue\n",
    "\n",
    "                    flat_tokens: list[str] = []\n",
    "                    for ins in instrs:\n",
    "                        op = ins.get(\"operation\", \"\").strip()\n",
    "                        if not op:\n",
    "                            continue\n",
    "                        sent = _tokens_from_operation(op)\n",
    "                        if sent:\n",
    "                            flat_tokens.extend(sent)\n",
    "\n",
    "                    if not flat_tokens:\n",
    "                        continue\n",
    "\n",
    "                    tokenized_line = \" \".join(flat_tokens)\n",
    "                    key = f\"{file_base}::{fn}\"\n",
    "                    arch_data[arch][key] = (file_base, fn, tokenized_line)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"{file_base} 讀取失敗，跳過: {e}\")\n",
    "    return arch_data\n",
    "\n",
    "\n",
    "def load_pretrained_model():\n",
    "    \"\"\"載入預訓練的模型和tokenizer\"\"\"\n",
    "    model_path = \"/home/tommy/Project/PcodeBERT/outputs/models/pretrain\"\n",
    "    \n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = RobertaForMaskedLM.from_pretrained(model_path)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded successfully on device: {device}\")\n",
    "    return model, tokenizer, device\n",
    "\n",
    "def get_sentence_embedding(sentence, model, tokenizer, device):\n",
    "    \"\"\"對單個sentence生成embedding\"\"\"\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # model.roberta(**inputs) -> model(**inputs) for base model\n",
    "        outputs = model(**inputs, output_hidden_states=True) \n",
    "        # 使用最後一層 hidden state 的 [CLS] token embedding\n",
    "        embedding = outputs.hidden_states[-1][:, 0, :].cpu().numpy()\n",
    "    \n",
    "    return embedding[0]\n",
    "\n",
    "\n",
    "def extract_and_vectorize_with_bert():\n",
    "    input_folders = [\n",
    "        \"/home/tommy/Project/PcodeBERT/outputs/align_sentences\"\n",
    "    ]\n",
    "\n",
    "    output_path = \"/home/tommy/Project/PcodeBERT/outputs/alignment_vector\"\n",
    "    archs = [\"mips_32\", \"arm_32\", \"x86_64\"]\n",
    "    print(\"Loading BERT model...\")\n",
    "    model, tokenizer, device = load_pretrained_model()\n",
    "\n",
    "    # 每個來源資料夾都跑一次\n",
    "    arch_datasets = []\n",
    "    print(\"Loading data from folders...\")\n",
    "    for folder in input_folders:\n",
    "        arch_datasets.append(load_data_from_folder(folder, archs))\n",
    "\n",
    "    # 找所有來源的交集\n",
    "    print(\"Finding common keys...\")\n",
    "    common_keys = None\n",
    "    for arch_data in arch_datasets:\n",
    "        if common_keys is None:\n",
    "            common_keys = set(arch_data[\"mips_32\"]) & set(arch_data[\"arm_32\"]) & set(arch_data[\"x86_64\"])\n",
    "        else:\n",
    "            common_keys &= set(arch_data[\"mips_32\"]) & set(arch_data[\"arm_32\"]) & set(arch_data[\"x86_64\"])\n",
    "\n",
    "    common_keys = list(common_keys)\n",
    "    random.shuffle(common_keys)\n",
    "\n",
    "    # 平均切給不同來源\n",
    "    n = len(input_folders)\n",
    "    chunk_size = len(common_keys) // n\n",
    "    key_groups = [common_keys[i*chunk_size:(i+1)*chunk_size] for i in range(n)]\n",
    "    \n",
    "\n",
    "    print(\"Generating embeddings...\")\n",
    "    samples = []\n",
    "    for arch_data, keys in zip(arch_datasets, key_groups):\n",
    "        for i, key in enumerate(keys):\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Processing item {i+1}/{len(keys)}...\")\n",
    "            \n",
    "            _, _, x86_op = arch_data[\"x86_64\"][key]\n",
    "            _, _, arm_op = arch_data[\"arm_32\"][key]\n",
    "            \n",
    "            vec_x = get_sentence_embedding(x86_op, model, tokenizer, device)\n",
    "            vec_a = get_sentence_embedding(arm_op, model, tokenizer, device)\n",
    "            \n",
    "            samples.append((x86_op, arm_op, vec_x, vec_a, 1))\n",
    "    \n",
    "    print(\"Saving results...\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    pk_file = os.path.join(output_path, \"train_arm_vector_mix_bert.pickle\")\n",
    "    with open(pk_file, \"wb\") as f:\n",
    "        pickle.dump([(vec_x, vec_a, label) for _, _, vec_x, vec_a, label in samples], f)\n",
    "\n",
    "    csv_file = os.path.join(output_path, \"train_arm_op_mix_bert.csv\")\n",
    "    with open(csv_file, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"x86_op\", \"arm_op\", \"label\"])\n",
    "        for x86_op, arm_op, _, _, label in samples:\n",
    "            writer.writerow([x86_op, arm_op, label])\n",
    "            \n",
    "    \n",
    "    print(f\"\\n已生成 {pk_file} & {csv_file}，共 {len(samples)} 筆樣本\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_and_vectorize_with_bert()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PcodeBERT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
