{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f2d1b0",
   "metadata": {},
   "source": [
    "TSNE Check Embeddings Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ac74ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Union\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# 忽略 tqdm 的 FutureWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def extract_vectors_from_gpickle(csv_file_path: Union[str, Path], root_dir: Union[str, Path], cpu_filter: List[str]):\n",
    "    \"\"\"\n",
    "    從 CSV 讀取檔案資訊，並從 .gpickle 檔案中提取向量\n",
    "    \"\"\"\n",
    "    root_path = Path(root_dir)\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    filtered_df = df[df['CPU'].isin(cpu_filter)]\n",
    "    \n",
    "    print(f\"原始資料筆數：{len(df)}\")\n",
    "    print(f\"篩選後資料筆數：{len(filtered_df)}\")\n",
    "    \n",
    "    vectors = []\n",
    "    cpus = []\n",
    "    families = []\n",
    "    \n",
    "    for _, row in tqdm(filtered_df.iterrows(), total=len(filtered_df), desc=\"Processing Gpickle files\"):\n",
    "        file_name = row['file_name']\n",
    "        cpu = row['CPU']\n",
    "        family = row['family']\n",
    "        \n",
    "        prefix = file_name[:2]\n",
    "        path = root_path / prefix / f\"{file_name}.gpickle\"\n",
    "        \n",
    "        if path.exists():\n",
    "            try:\n",
    "                with open(path, \"rb\") as fp:\n",
    "                    data = pickle.load(fp)\n",
    "                \n",
    "                if 'node_embeddings' in data and data['node_embeddings']:\n",
    "                    node_embeddings = data['node_embeddings']\n",
    "                    node_vectors = [np.array(emb) for emb in node_embeddings.values() if len(emb) == 256]\n",
    "                    \n",
    "                    if node_vectors:\n",
    "                        avg_vector = np.mean(node_vectors, axis=0)\n",
    "                        vectors.append(avg_vector)\n",
    "                        cpus.append(cpu)\n",
    "                        families.append(family)\n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"[Error] Load Gpickle Failed {path}: {e}\")\n",
    "            \n",
    "    return np.array(vectors), cpus, families\n",
    "\n",
    "def visualize_and_analyze_tsne(vectors, cpus, families):\n",
    "    \"\"\"\n",
    "    執行 t-SNE，視覺化並輸出數值化分析報告。\n",
    "    \"\"\"\n",
    "    # t-SNE 降維\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=15)\n",
    "    vectors_2d = tsne.fit_transform(vectors)\n",
    "    \n",
    "    # 視覺化\n",
    "    cpu_markers = {'ARM-32': 's', 'AMD X86-64': 'o'}\n",
    "    family_colors = plt.cm.get_cmap('tab10', len(set(families)))\n",
    "    family_color_map = {family: family_colors(i) for i, family in enumerate(sorted(list(set(families))))}\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    unique_cpus = sorted(list(set(cpus)))\n",
    "    unique_families = sorted(list(set(families)))\n",
    "\n",
    "    for cpu in unique_cpus:\n",
    "        for family in unique_families:\n",
    "            mask = [(c == cpu and f == family) for c, f in zip(cpus, families)]\n",
    "            if any(mask):\n",
    "                current_vectors = vectors_2d[mask]\n",
    "                ax.scatter(current_vectors[:, 0], current_vectors[:, 1],\n",
    "                            c=[family_color_map.get(family)], marker=cpu_markers.get(cpu, 'o'),\n",
    "                            alpha=0.8, s=80, label=f'{cpu} - {family}')\n",
    "    \n",
    "    ax.set_title('t-SNE Visualization by CPU (Shape) and Family (Color)')\n",
    "    ax.set_xlabel('t-SNE 1')\n",
    "    ax.set_ylabel('t-SNE 2')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 數值化分析\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Numerical Analysis: Cluster Centroids & Distances\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    cluster_centroids = {}\n",
    "    labels = []\n",
    "    \n",
    "    for cpu in unique_cpus:\n",
    "        for family in unique_families:\n",
    "            mask = [(c == cpu and f == family) for c, f in zip(cpus, families)]\n",
    "            if any(mask):\n",
    "                current_vectors = vectors_2d[mask]\n",
    "                centroid = np.mean(current_vectors, axis=0)\n",
    "                cluster_label = f'{cpu}-{family}'\n",
    "                cluster_centroids[cluster_label] = centroid\n",
    "                labels.append(cluster_label)\n",
    "    \n",
    "    centroids_array = np.array(list(cluster_centroids.values()))\n",
    "    \n",
    "    # 計算歐式距離矩陣\n",
    "    distances = euclidean_distances(centroids_array)\n",
    "    \n",
    "    # 建立 DataFrame 進行視覺化輸出\n",
    "    distance_df = pd.DataFrame(distances, index=labels, columns=labels).round(2)\n",
    "    \n",
    "    print(\"\\nCluster Centroid Coordinates (t-SNE 2D):\")\n",
    "    centroids_df = pd.DataFrame(centroids_array, index=labels, columns=['t-SNE 1', 't-SNE 2']).round(2)\n",
    "    print(centroids_df)\n",
    "    \n",
    "    print(\"\\nEuclidean Distance Between Clusters:\")\n",
    "    print(distance_df)\n",
    "\n",
    "# 主程式\n",
    "def main():\n",
    "    csv_file_path = \"/home/tommy/Project/PcodeBERT/dataset/csv/base_dataset_filtered_v2.csv\"\n",
    "    gpickle_dir = \"/home/tommy/Project/PcodeBERT/outputs/embeddings_adapted\"\n",
    "    \n",
    "    target_cpus = ['AMD X86-64', 'ARM-32']\n",
    "\n",
    "    print(\"提取向量和家族資訊...\")\n",
    "    vectors, cpus, families = extract_vectors_from_gpickle(\n",
    "        csv_file_path, gpickle_dir, target_cpus\n",
    "    )\n",
    "    \n",
    "    print(f\"總共載入 {len(vectors)} 個向量\")\n",
    "    if len(vectors) > 0:\n",
    "        if len(vectors) > 5000:\n",
    "            print(\"數據量較大，進行抽樣以加速 t-SNE 運算。\")\n",
    "            import random\n",
    "            sample_size = 5000\n",
    "            indices = random.sample(range(len(vectors)), sample_size)\n",
    "            vectors_sampled = vectors[indices]\n",
    "            cpus_sampled = [cpus[i] for i in indices]\n",
    "            families_sampled = [families[i] for i in indices]\n",
    "        else:\n",
    "            vectors_sampled = vectors\n",
    "            cpus_sampled = cpus\n",
    "            families_sampled = families\n",
    "        \n",
    "        print(\"執行 t-SNE 並創建視覺化...\")\n",
    "        visualize_and_analyze_tsne(vectors_sampled, cpus_sampled, families_sampled)\n",
    "    else:\n",
    "        print(\"沒有找到符合條件的數據。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b82b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch_geometric.data import Data\n",
    "import warnings\n",
    "\n",
    "# 忽略 scikit-learn 的 FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
    "\n",
    "def get_gnn_config():\n",
    "    BASE_PATH = \"/home/tommy/Project/PcodeBERT\"\n",
    "    \n",
    "    config = {\n",
    "        \"source_cpus\": [\"AMD X86-64\"],     \n",
    "        \"target_cpus\": [\"ARM-32\"],        \n",
    "        \n",
    "        \"csv_path\": f\"{BASE_PATH}/dataset/csv/base_dataset_filtered_v2.csv\",\n",
    "        \"graph_dir\": f\"{BASE_PATH}/outputs/embeddings\",\n",
    "        \"cache_file\": f\"{BASE_PATH}/outputs/cache/gnn_data_by_family_label.pkl\",\n",
    "        \"model_output_dir\": f\"{BASE_PATH}/outputs/models/gnn\",\n",
    "        \n",
    "        \"batch_size\": 32,\n",
    "        \"hidden_channels\": 64,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"epochs\": 200,\n",
    "        \"patience\": 20,\n",
    "        \n",
    "        \"seeds\": [42, 123, 2025, 31415, 8888],\n",
    "        \"device\": \"cuda\"\n",
    "    }\n",
    "    \n",
    "    return config\n",
    "\n",
    "def load_graphs_from_df(df, graph_dir, label_col='label'):\n",
    "    \"\"\"\n",
    "    從 DataFrame 載入圖資料。\n",
    "    新增了 label_col 參數來決定使用哪一欄作為標籤。\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        file_name = row['file_name']\n",
    "        prefix = file_name[:2]\n",
    "        label = row[label_col]\n",
    "        graph_path = Path(graph_dir) / prefix / f\"{file_name}.gpickle\"\n",
    "\n",
    "        if not graph_path.exists():\n",
    "            continue\n",
    "            \n",
    "        with open(graph_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        node_embeddings = data['node_embeddings']\n",
    "        if not node_embeddings:\n",
    "            continue\n",
    "        \n",
    "        embeddings = [list(emb) for emb in node_embeddings.values()]\n",
    "        x = torch.tensor(embeddings, dtype=torch.float)\n",
    "        \n",
    "        num_nodes = len(embeddings)\n",
    "        edge_list = []\n",
    "        for i in range(num_nodes - 1):\n",
    "            edge_list.extend([[i, i+1], [i+1, i]])\n",
    "        \n",
    "        if edge_list:\n",
    "            edge_index = torch.tensor(edge_list, dtype=torch.long).t()\n",
    "        else:\n",
    "            edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
    "        \n",
    "        graph_data = Data(x=x, edge_index=edge_index)\n",
    "        graphs.append(graph_data)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return graphs, labels\n",
    "\n",
    "def load_cross_arch_data_with_family_label(csv_path, graph_dir, source_cpus, target_cpus, cache_file, val_size=0.2, random_state=42, force_reload=False):\n",
    "    \"\"\"\n",
    "    載入跨架構的圖資料，並以 'family' 作為標籤。\n",
    "    \"\"\"\n",
    "    if force_reload and os.path.exists(cache_file):\n",
    "        os.remove(cache_file)\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Loading cached data from: {cache_file}\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            cached_data = pickle.load(f)\n",
    "        return (cached_data['train_graphs'], \n",
    "                cached_data['val_graphs'],\n",
    "                cached_data['test_graphs'], \n",
    "                cached_data['label_encoder'], \n",
    "                cached_data['num_classes'])\n",
    "    \n",
    "    print(\"Loading CSV data...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    train_df = df[df['CPU'].isin(source_cpus)]\n",
    "    test_df = df[df['CPU'].isin(target_cpus)]\n",
    "    \n",
    "    print(f\"Training data: {len(train_df)} samples (architectures: {source_cpus})\")\n",
    "    print(f\"Test data: {len(test_df)} samples (architectures: {target_cpus})\")\n",
    "    \n",
    "    train_graphs, train_labels = load_graphs_from_df(train_df, graph_dir, label_col='family')\n",
    "    test_graphs, test_labels = load_graphs_from_df(test_df, graph_dir, label_col='family')\n",
    "    \n",
    "    train_graphs, val_graphs, train_labels, val_labels = train_test_split(\n",
    "        train_graphs, train_labels, test_size=val_size, \n",
    "        stratify=train_labels, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    all_labels = train_labels + val_labels + test_labels\n",
    "    label_encoder.fit(all_labels)\n",
    "    \n",
    "    encoded_train_labels = label_encoder.transform(train_labels)\n",
    "    encoded_val_labels = label_encoder.transform(val_labels)\n",
    "    encoded_test_labels = label_encoder.transform(test_labels)\n",
    "    \n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    \n",
    "    for i, data in enumerate(train_graphs):\n",
    "        data.y = torch.tensor(encoded_train_labels[i], dtype=torch.long)\n",
    "        \n",
    "    for i, data in enumerate(val_graphs):\n",
    "        data.y = torch.tensor(encoded_val_labels[i], dtype=torch.long)\n",
    "        \n",
    "    for i, data in enumerate(test_graphs):\n",
    "        data.y = torch.tensor(encoded_test_labels[i], dtype=torch.long)\n",
    "\n",
    "    os.makedirs(os.path.dirname(cache_file), exist_ok=True)\n",
    "    cache_data = {\n",
    "        'train_graphs': train_graphs,\n",
    "        'val_graphs': val_graphs,\n",
    "        'test_graphs': test_graphs,\n",
    "        'label_encoder': label_encoder,\n",
    "        'num_classes': num_classes\n",
    "    }\n",
    "    \n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(cache_data, f)\n",
    "    \n",
    "    print(f\"Data has been cached to: {cache_file}\")\n",
    "    return train_graphs, val_graphs, test_graphs, label_encoder, num_classes\n",
    "\n",
    "def analyze_dataset_by_family(graphs, label_encoder):\n",
    "    \"\"\"\n",
    "    Analyzes the node distribution for each family and returns a dictionary of stats.\n",
    "    \"\"\"\n",
    "    family_stats = {}\n",
    "    if not graphs:\n",
    "        return family_stats\n",
    "\n",
    "    # Group graphs by family\n",
    "    family_groups = defaultdict(list)\n",
    "    for g in graphs:\n",
    "        family = label_encoder.inverse_transform([int(g.y)])[0]\n",
    "        family_groups[family].append(g.num_nodes)\n",
    "\n",
    "    for family, node_counts in family_groups.items():\n",
    "        family_stats[family] = {\n",
    "            \"Total Graphs\": len(node_counts),\n",
    "            \"Avg Nodes\": np.mean(node_counts),\n",
    "            \"Median Nodes\": np.median(node_counts),\n",
    "            \"Max Nodes\": np.max(node_counts),\n",
    "            \"Min Nodes\": np.min(node_counts)\n",
    "        }\n",
    "    return family_stats\n",
    "\n",
    "def compare_datasets(train_graphs, val_graphs, test_graphs, label_encoder):\n",
    "    \"\"\"\n",
    "    Compares key statistics of training, validation, and test datasets.\n",
    "    \"\"\"\n",
    "    # First, print the overall comparison report\n",
    "    train_stats = analyze_dataset_by_family(train_graphs, label_encoder)\n",
    "    test_stats = analyze_dataset_by_family(test_graphs, label_encoder)\n",
    "\n",
    "    all_families = sorted(list(set(train_stats.keys()) | set(test_stats.keys())))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Family-wise Node Count Comparison\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for family in all_families:\n",
    "        train_data = train_stats.get(family, {\"Total Graphs\": 0, \"Avg Nodes\": 0, \"Median Nodes\": 0, \"Max Nodes\": 0, \"Min Nodes\": 0})\n",
    "        test_data = test_stats.get(family, {\"Total Graphs\": 0, \"Avg Nodes\": 0, \"Median Nodes\": 0, \"Max Nodes\": 0, \"Min Nodes\": 0})\n",
    "        \n",
    "        print(f\"\\n--- Family: {family} ---\")\n",
    "        \n",
    "        comparison_df = pd.DataFrame({\n",
    "            \"Metric\": [\"Total Graphs\", \"Avg Nodes\", \"Median Nodes\", \"Max Nodes\", \"Min Nodes\"],\n",
    "            \"Training Set\": [train_data[\"Total Graphs\"], f\"{train_data['Avg Nodes']:.2f}\", train_data[\"Median Nodes\"], train_data[\"Max Nodes\"], train_data[\"Min Nodes\"]],\n",
    "            \"Test Set\": [test_data[\"Total Graphs\"], f\"{test_data['Avg Nodes']:.2f}\", test_data[\"Median Nodes\"], test_data[\"Max Nodes\"], test_data[\"Min Nodes\"]]\n",
    "        })\n",
    "        print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    gnn_config = get_gnn_config()\n",
    "    \n",
    "    train_graphs, val_graphs, test_graphs, label_encoder, num_classes = load_cross_arch_data_with_family_label(\n",
    "        csv_path=gnn_config[\"csv_path\"],\n",
    "        graph_dir=gnn_config[\"graph_dir\"],\n",
    "        source_cpus=gnn_config[\"source_cpus\"],\n",
    "        target_cpus=gnn_config[\"target_cpus\"],\n",
    "        cache_file=gnn_config[\"cache_file\"],\n",
    "        force_reload=False\n",
    "    )\n",
    "\n",
    "    compare_datasets(train_graphs, val_graphs, test_graphs, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f8f5e",
   "metadata": {},
   "source": [
    "Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1c1a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1, Count: 68688\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = \"/home/tommy/Project/PcodeBERT/outputs/alignment_sentences/train_x86_64_arm_32_embeddings.pickle\"\n",
    "\n",
    "with open(file_path, \"rb\") as fp:\n",
    "    arch_data = pickle.load(fp)\n",
    "\n",
    "\n",
    "#check label distribution\n",
    "#data format is list of (vec1, vec2, label)\n",
    "label_count = {}\n",
    "for item in arch_data:\n",
    "    label = item[2]\n",
    "    if label not in label_count:\n",
    "        label_count[label] = 0\n",
    "    label_count[label] += 1\n",
    "\n",
    "# Print label distribution\n",
    "for label, count in label_count.items():\n",
    "    print(f\"Label: {label}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ac1b88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "計算中: 100%|██████████| 68688/68688 [06:22<00:00, 179.71it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均 similarity score: 0.1724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "path = \"/home/tommy/Project/PcodeBERT/outputs/alignment_sentences/train_x86_64_arm_32_embeddings.pickle\"\n",
    "\n",
    "with open(path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === 定義 similarity 函式 ===\n",
    "def similarity_score(x1, x2):\n",
    "    dist = torch.norm(x1 - x2, dim=1)  \n",
    "    return 1 / (1 + dist)             \n",
    "\n",
    "# === 批次計算平均 similarity ===\n",
    "scores = []\n",
    "for v1, v2, _ in tqdm(data, desc=\"計算中\"):\n",
    "    t1 = torch.tensor(v1, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    t2 = torch.tensor(v2, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    score = similarity_score(t1, t2)\n",
    "    scores.append(score.item())\n",
    "\n",
    "avg_score = np.mean(scores)\n",
    "print(f\"平均 similarity score: {avg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16541b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_path = \"/home/tommy/Project/PcodeBERT/outputs/alignment_vector/train_arm_vector_mix_bert.pickle\"\n",
    "output_path = \"/home/tommy/Project/PcodeBERT/outputs/alignment_vector/train_arm_vector_contrastive_bert.pickle\"\n",
    "\n",
    "with open(input_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === similarity 函式 ===\n",
    "def similarity_score(x1, x2):\n",
    "    dist = torch.norm(x1 - x2, dim=1)  # L2 distance\n",
    "    return 1 / (1 + dist)\n",
    "\n",
    "# === 計算所有 similarity score ===\n",
    "scored_data = []\n",
    "for v1, v2, label in tqdm(data, desc=\"計算相似度\"):\n",
    "    t1 = torch.tensor(v1, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    t2 = torch.tensor(v2, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    score = similarity_score(t1, t2).item()\n",
    "    scored_data.append((v1, v2, label, score))\n",
    "\n",
    "# === 依照相似度排序 ===\n",
    "scored_data.sort(key=lambda x: x[3], reverse=True)\n",
    " \n",
    "\n",
    "# === 分成兩半 ===\n",
    "half = len(scored_data) // 2\n",
    "positive_samples = [(v1, v2, 1) for v1, v2, _, _ in scored_data[:half]]\n",
    "negative_candidates = [ (v1, v2) for v1, v2, _, _ in scored_data[half:] ]\n",
    "\n",
    "print(f\"[INFO] 正樣本數: {len(positive_samples)}\")\n",
    "print(f\"[INFO] 負樣本候選數: {len(negative_candidates)}\")\n",
    "\n",
    "# === 打亂後配對建立負樣本 ===\n",
    "all_vec1 = [v1 for v1, _ in negative_candidates]\n",
    "all_vec2 = [v2 for _, v2 in negative_candidates]\n",
    "random.shuffle(all_vec2)\n",
    "\n",
    "negative_samples = [(v1, v2, 0) for v1, v2 in zip(all_vec1, all_vec2)]\n",
    "\n",
    "#print sample of negative samples\n",
    "print(\"\\n[INFO] 負樣本範例:\")\n",
    "for v1, v2, _ in negative_samples[:5]:\n",
    "    print(f\"  - {v1} <-> {v2}\")\n",
    "\n",
    "# === 合併正負樣本 ===\n",
    "final_dataset = positive_samples + negative_samples\n",
    "random.shuffle(final_dataset)\n",
    "\n",
    "# === 儲存 ===\n",
    "with open(output_path, \"wb\") as f:\n",
    "    pickle.dump(final_dataset, f)\n",
    "\n",
    "print(f\"[INFO] 原始樣本數: {len(data)}\")\n",
    "print(f\"[INFO] 新資料集樣本數: {len(final_dataset)}\")\n",
    "print(f\"[INFO] 已儲存至: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b39c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check label 1 and label 0 simiarity score\n",
    "import pickle \n",
    "\n",
    "with open(\"/home/tommy/Project/PcodeBERT/outputs/alignment_vector/train_arm_vector_contrastive_bert.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "#check label distribution\n",
    "from collections import Counter\n",
    "labels = [label for _, _, label in data]\n",
    "label_counts = Counter(labels)\n",
    "print(\"Label distribution:\", label_counts)\n",
    "\n",
    "for v1, v2, label in data:\n",
    "    if label == 1:\n",
    "        t1 = torch.tensor(v1, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        t2 = torch.tensor(v2, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        pos_score = similarity_score(t1, t2)\n",
    "        print(f\"Positive pair similarity score: {pos_score.item():.4f}\")\n",
    "        break\n",
    "for v1, v2, label in data:\n",
    "    if label == 0:\n",
    "        t1 = torch.tensor(v1, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        t2 = torch.tensor(v2, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        neg_score = similarity_score(t1, t2)\n",
    "        print(f\"Negative pair similarity score: {neg_score.item():.4f}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67638f",
   "metadata": {},
   "source": [
    "Check family embedding across architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed0279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# --- 配置路徑 ---\n",
    "csv_path = \"/home/tommy/Project/PcodeBERT/dataset/csv/base_dataset_filtered.csv\"\n",
    "embedding_path = \"/home/tommy/Project/PcodeBERT/outputs/embeddings\"\n",
    "# 相似度比較的樣本數量\n",
    "NUM_SAMPLES_TO_COMPARE = 5\n",
    "# 每個樣本找出的最相似函式數量\n",
    "TOP_K = 5\n",
    "\n",
    "# --- 輔助函式 ---\n",
    "\n",
    "def load_embeddings(func_id: str, arch: str) -> np.ndarray | None:\n",
    "    \"\"\"載入特定函式ID和架構的嵌入向量。\"\"\"\n",
    "    # 假設嵌入檔案的命名格式為 {func_id}_{arch}.npy\n",
    "    # 範例: 0000_AMD X86-64.npy\n",
    "    safe_arch = arch.replace(' ', '_')  # 替換空格以適應檔案名\n",
    "    embedding_file = f\"{func_id}_{safe_arch}.npy\"\n",
    "    full_path = os.path.join(embedding_path, safe_arch, embedding_file)\n",
    "    \n",
    "    # 檢查檔案是否存在\n",
    "    if not os.path.exists(full_path):\n",
    "        # print(f\"Warning: Embedding file not found for {func_id} ({arch}) at {full_path}\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # 嵌入向量檔案通常儲存為 numpy array\n",
    "        return np.load(full_path)\n",
    "    except Exception as e:\n",
    "        # print(f\"Error loading embedding for {func_id} ({arch}): {e}\")\n",
    "        return None\n",
    "\n",
    "def find_most_similar(query_embed: np.ndarray, target_df: pd.DataFrame, target_arch: str, k: int = 5) -> list[tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    計算查詢向量與目標資料集中所有函式的歐式距離，並返回最相似的 K 個結果。\n",
    "    \"\"\"\n",
    "    target_embeddings = []\n",
    "    target_func_ids = []\n",
    "    \n",
    "    # 1. 批次載入目標架構的嵌入向量\n",
    "    for _, row in target_df.iterrows():\n",
    "        func_id = str(row['FunctionID'])\n",
    "        embed = load_embeddings(func_id, target_arch)\n",
    "        if embed is not None:\n",
    "            # PcodeBERT 輸出的嵌入向量通常是 (1, N) 或 (N,) 維度，確保是 (N,)\n",
    "            target_embeddings.append(embed.flatten())\n",
    "            target_func_ids.append(func_id)\n",
    "\n",
    "    if not target_embeddings:\n",
    "        return []\n",
    "\n",
    "    target_embeddings_matrix = np.array(target_embeddings)\n",
    "    \n",
    "    # 2. 計算歐式距離 (Euclidean Distance)\n",
    "    # distance matrix D[i][j] 是 query_embed[i] 和 target_embeddings_matrix[j] 的距離\n",
    "    # 這裡只有一個查詢向量，所以結果是一個 (1, M) 的矩陣\n",
    "    distances = euclidean_distances(query_embed.reshape(1, -1), target_embeddings_matrix)[0]\n",
    "\n",
    "    # 3. 排序並取出前 K 個最小距離（最相似）\n",
    "    # argsort 返回排序後的索引\n",
    "    sorted_indices = np.argsort(distances)\n",
    "    \n",
    "    results = []\n",
    "    for i in sorted_indices[:k]:\n",
    "        func_id = target_func_ids[i]\n",
    "        distance = distances[i]\n",
    "        results.append((func_id, distance))\n",
    "        \n",
    "    return results\n",
    "\n",
    "# --- 主要邏輯 ---\n",
    "\n",
    "def run_similarity_analysis():\n",
    "    \"\"\"執行跨架構相似性分析\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file not found at {csv_path}\")\n",
    "        return\n",
    "    \n",
    "    # 定義要比較的兩個群組\n",
    "    # 假設 ARM 群組使用 'ARM' 架構\n",
    "    ARCH_A = 'AMD X86-64'\n",
    "    ARCH_B = 'ARM-32' # 或其他 ARM 架構名稱，請依據您的 CSV 調整\n",
    "    FAMILY = 'mirai'\n",
    "    \n",
    "    # 過濾 Mirai 樣本\n",
    "    df_mirai = df[df['family'] == FAMILY].copy()\n",
    "    \n",
    "    # 1. 篩選 X86 Mirai 樣本作為查詢集 (Query Set)\n",
    "    query_df = df_mirai[df_mirai['CPU'] == ARCH_A].head(NUM_SAMPLES_TO_COMPARE)\n",
    "    print(f\"--- 查詢集 ({ARCH_A} {FAMILY}) 樣本數: {len(query_df)} ---\")\n",
    "    # print(query_df[['FunctionID', 'CPU', 'filename']].to_string(index=False))\n",
    "\n",
    "    # 2. 篩選 ARM Mirai 樣本作為目標集 (Target Set)\n",
    "    target_df = df_mirai[df_mirai['CPU'] == ARCH_B]\n",
    "    print(f\"--- 目標集 ({ARCH_B} {FAMILY}) 樣本數: {len(target_df)} ---\")\n",
    "    if target_df.empty:\n",
    "        print(f\"Error: No samples found for {ARCH_B} {FAMILY}. Please check the CPU name.\")\n",
    "        return\n",
    "\n",
    "    # 3. 執行相似度比較\n",
    "    print(\"\\n--- 跨架構相似度比較結果 (X86 -> ARM) ---\")\n",
    "    results = []\n",
    "\n",
    "    for _, query_row in query_df.iterrows():\n",
    "        query_func_id = str(query_row['FunctionID'])\n",
    "        query_arch = query_row['CPU']\n",
    "        \n",
    "        # 載入查詢函式的嵌入向量\n",
    "        query_embed = load_embeddings(query_func_id, query_arch)\n",
    "        \n",
    "        if query_embed is None:\n",
    "            continue\n",
    "        \n",
    "        # 查找最相似的 ARM 函式\n",
    "        similar_functions = find_most_similar(\n",
    "            query_embed.flatten(), \n",
    "            target_df, \n",
    "            target_arch=ARCH_B, \n",
    "            k=TOP_K\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            'Query_ID': query_func_id,\n",
    "            'Query_Arch': query_arch,\n",
    "            'Target_Arch': ARCH_B,\n",
    "            'Top_Similar': similar_functions\n",
    "        })\n",
    "        \n",
    "        print(f\"Query {query_func_id} ({query_arch}):\")\n",
    "        if similar_functions:\n",
    "            for target_id, dist in similar_functions:\n",
    "                print(f\"  -> Target {target_id} (Dist: {dist:.4f})\")\n",
    "        else:\n",
    "            print(\"  -> No similar functions found (可能是目標集嵌入向量載入失敗)\")\n",
    "\n",
    "    # 4. (可選) 後續分析共同點\n",
    "    # 這裡只輸出相似函式ID和距離，要分析共同點，\n",
    "    # 您需要進一步處理這些相似函式對的原始 P-code/彙編程式碼。\n",
    "    \n",
    "    # print(\"\\n--- 完整結果摘要 ---\")\n",
    "    # for res in results:\n",
    "    #     print(res)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_similarity_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63090b7",
   "metadata": {},
   "source": [
    "Chech Prtrain Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e3356",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RobertaForMaskedLM\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "file = \"/home/tommy/Project/PcodeBERT/outputs/gpickle_merged_adjusted_filtered/0e/0e0ebdff7ac27afdcf1d7e555e29002cbf3647cf408e5830ceb699c2ead5cd35.gpickle\"\n",
    "\n",
    "\n",
    "def load_pretrained_model():\n",
    "    \"\"\"載入預訓練的模型和tokenizer\"\"\"\n",
    "    model_path = \"/home/tommy/Project/PcodeBERT/checkpoints/model_epoch_50\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = RobertaForMaskedLM.from_pretrained(model_path)\n",
    "    # print(model.config)\n",
    "    # 設定device\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"模型總參數數量 (Total parameters): {total_params:,}\")\n",
    "    print(f\"可訓練參數數量 (Trainable parameters): {trainable_params:,}\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded successfully on device: {device}\")\n",
    "    return model, tokenizer, device\n",
    "\n",
    "model, tokenizer, device = load_pretrained_model()\n",
    "\n",
    "# with open(file, \"rb\") as fp:\n",
    "#     data = pickle.load(fp)\n",
    "#     input = tokenizer(data.nodes[\"0x1001a4b8L\"]['sentence'], return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "# print(\"0x1001a4b8L node sentence:\", data.nodes[\"0x1001a4b8L\"]['sentence'])\n",
    "# print(\"Input IDs:\", input['input_ids'])\n",
    "# print(\"Token Count:\", len(input['input_ids'][0]))\n",
    "\n",
    "# for node, node_data in data.nodes(data=True):\n",
    "#     sentence = node_data.get(\"sentence\")\n",
    "#     input = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "#     inputs = {k: v.to(device) for k, v in input.items()}\n",
    "#     #check sentence length\n",
    "#     input_length = inputs['input_ids'].shape[1]\n",
    "#     # print(f\"Node: {node}, Sentence length: {input_length}\")\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.roberta(**inputs)\n",
    "#         embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "#     print(f\"Node: {node}, Embedding shape: {embedding.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file = \"/home/tommy/Project/PcodeBERT/outputs/gpickle_merged_adjusted_filtered/0e/0e0ebdff7ac27afdcf1d7e555e29002cbf3647cf408e5830ceb699c2ead5cd35.gpickle\"\n",
    "\n",
    "with open(file, \"rb\") as fp:\n",
    "    data = pickle.load(fp)\n",
    "\n",
    "print(data)\n",
    "\n",
    "for node, node_data in data.nodes(data=True):\n",
    "    print(f\"Node {node}: {node_data}\")\n",
    "    embedding = node_data.get('sentence', None)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PcodeBERT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
