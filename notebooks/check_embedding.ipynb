{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f2d1b0",
   "metadata": {},
   "source": [
    "TSNE Check Embeddings Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ac74ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Union\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# 忽略 tqdm 的 FutureWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def extract_vectors_from_gpickle(csv_file_path: Union[str, Path], root_dir: Union[str, Path], cpu_filter: List[str]):\n",
    "    \"\"\"\n",
    "    從 CSV 讀取檔案資訊，並從 .gpickle 檔案中提取向量\n",
    "    \"\"\"\n",
    "    root_path = Path(root_dir)\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    filtered_df = df[df['CPU'].isin(cpu_filter)]\n",
    "    \n",
    "    print(f\"原始資料筆數：{len(df)}\")\n",
    "    print(f\"篩選後資料筆數：{len(filtered_df)}\")\n",
    "    \n",
    "    vectors = []\n",
    "    cpus = []\n",
    "    families = []\n",
    "    \n",
    "    for _, row in tqdm(filtered_df.iterrows(), total=len(filtered_df), desc=\"Processing Gpickle files\"):\n",
    "        file_name = row['file_name']\n",
    "        cpu = row['CPU']\n",
    "        family = row['family']\n",
    "        \n",
    "        prefix = file_name[:2]\n",
    "        path = root_path / prefix / f\"{file_name}.gpickle\"\n",
    "        \n",
    "        if path.exists():\n",
    "            try:\n",
    "                with open(path, \"rb\") as fp:\n",
    "                    data = pickle.load(fp)\n",
    "                \n",
    "                if 'node_embeddings' in data and data['node_embeddings']:\n",
    "                    node_embeddings = data['node_embeddings']\n",
    "                    node_vectors = [np.array(emb) for emb in node_embeddings.values() if len(emb) == 256]\n",
    "                    \n",
    "                    if node_vectors:\n",
    "                        avg_vector = np.mean(node_vectors, axis=0)\n",
    "                        vectors.append(avg_vector)\n",
    "                        cpus.append(cpu)\n",
    "                        families.append(family)\n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"[Error] Load Gpickle Failed {path}: {e}\")\n",
    "            \n",
    "    return np.array(vectors), cpus, families\n",
    "\n",
    "def visualize_and_analyze_tsne(vectors, cpus, families):\n",
    "    \"\"\"\n",
    "    執行 t-SNE，視覺化並輸出數值化分析報告。\n",
    "    \"\"\"\n",
    "    # t-SNE 降維\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=15)\n",
    "    vectors_2d = tsne.fit_transform(vectors)\n",
    "    \n",
    "    # 視覺化\n",
    "    cpu_markers = {'ARM-32': 's', 'AMD X86-64': 'o'}\n",
    "    family_colors = plt.cm.get_cmap('tab10', len(set(families)))\n",
    "    family_color_map = {family: family_colors(i) for i, family in enumerate(sorted(list(set(families))))}\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    unique_cpus = sorted(list(set(cpus)))\n",
    "    unique_families = sorted(list(set(families)))\n",
    "\n",
    "    for cpu in unique_cpus:\n",
    "        for family in unique_families:\n",
    "            mask = [(c == cpu and f == family) for c, f in zip(cpus, families)]\n",
    "            if any(mask):\n",
    "                current_vectors = vectors_2d[mask]\n",
    "                ax.scatter(current_vectors[:, 0], current_vectors[:, 1],\n",
    "                            c=[family_color_map.get(family)], marker=cpu_markers.get(cpu, 'o'),\n",
    "                            alpha=0.8, s=80, label=f'{cpu} - {family}')\n",
    "    \n",
    "    ax.set_title('t-SNE Visualization by CPU (Shape) and Family (Color)')\n",
    "    ax.set_xlabel('t-SNE 1')\n",
    "    ax.set_ylabel('t-SNE 2')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 數值化分析\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Numerical Analysis: Cluster Centroids & Distances\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    cluster_centroids = {}\n",
    "    labels = []\n",
    "    \n",
    "    for cpu in unique_cpus:\n",
    "        for family in unique_families:\n",
    "            mask = [(c == cpu and f == family) for c, f in zip(cpus, families)]\n",
    "            if any(mask):\n",
    "                current_vectors = vectors_2d[mask]\n",
    "                centroid = np.mean(current_vectors, axis=0)\n",
    "                cluster_label = f'{cpu}-{family}'\n",
    "                cluster_centroids[cluster_label] = centroid\n",
    "                labels.append(cluster_label)\n",
    "    \n",
    "    centroids_array = np.array(list(cluster_centroids.values()))\n",
    "    \n",
    "    # 計算歐式距離矩陣\n",
    "    distances = euclidean_distances(centroids_array)\n",
    "    \n",
    "    # 建立 DataFrame 進行視覺化輸出\n",
    "    distance_df = pd.DataFrame(distances, index=labels, columns=labels).round(2)\n",
    "    \n",
    "    print(\"\\nCluster Centroid Coordinates (t-SNE 2D):\")\n",
    "    centroids_df = pd.DataFrame(centroids_array, index=labels, columns=['t-SNE 1', 't-SNE 2']).round(2)\n",
    "    print(centroids_df)\n",
    "    \n",
    "    print(\"\\nEuclidean Distance Between Clusters:\")\n",
    "    print(distance_df)\n",
    "\n",
    "# 主程式\n",
    "def main():\n",
    "    csv_file_path = \"/home/tommy/Project/PcodeBERT/dataset/csv/base_dataset_filtered_v2.csv\"\n",
    "    gpickle_dir = \"/home/tommy/Project/PcodeBERT/outputs/embeddings\"\n",
    "    \n",
    "    target_cpus = ['AMD X86-64', 'ARM-32']\n",
    "\n",
    "    print(\"提取向量和家族資訊...\")\n",
    "    vectors, cpus, families = extract_vectors_from_gpickle(\n",
    "        csv_file_path, gpickle_dir, target_cpus\n",
    "    )\n",
    "    \n",
    "    print(f\"總共載入 {len(vectors)} 個向量\")\n",
    "    if len(vectors) > 0:\n",
    "        if len(vectors) > 5000:\n",
    "            print(\"數據量較大，進行抽樣以加速 t-SNE 運算。\")\n",
    "            import random\n",
    "            sample_size = 5000\n",
    "            indices = random.sample(range(len(vectors)), sample_size)\n",
    "            vectors_sampled = vectors[indices]\n",
    "            cpus_sampled = [cpus[i] for i in indices]\n",
    "            families_sampled = [families[i] for i in indices]\n",
    "        else:\n",
    "            vectors_sampled = vectors\n",
    "            cpus_sampled = cpus\n",
    "            families_sampled = families\n",
    "        \n",
    "        print(\"執行 t-SNE 並創建視覺化...\")\n",
    "        visualize_and_analyze_tsne(vectors_sampled, cpus_sampled, families_sampled)\n",
    "    else:\n",
    "        print(\"沒有找到符合條件的數據。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b82b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch_geometric.data import Data\n",
    "import warnings\n",
    "\n",
    "# 忽略 scikit-learn 的 FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
    "\n",
    "def get_gnn_config():\n",
    "    BASE_PATH = \"/home/tommy/Project/PcodeBERT\"\n",
    "    \n",
    "    config = {\n",
    "        \"source_cpus\": [\"AMD X86-64\"],     \n",
    "        \"target_cpus\": [\"ARM-32\"],        \n",
    "        \n",
    "        \"csv_path\": f\"{BASE_PATH}/dataset/csv/base_dataset_filtered_v2.csv\",\n",
    "        \"graph_dir\": f\"{BASE_PATH}/outputs/embeddings\",\n",
    "        \"cache_file\": f\"{BASE_PATH}/outputs/cache/gnn_data_by_family_label.pkl\",\n",
    "        \"model_output_dir\": f\"{BASE_PATH}/outputs/models/gnn\",\n",
    "        \n",
    "        \"batch_size\": 32,\n",
    "        \"hidden_channels\": 64,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"epochs\": 200,\n",
    "        \"patience\": 20,\n",
    "        \n",
    "        \"seeds\": [42, 123, 2025, 31415, 8888],\n",
    "        \"device\": \"cuda\"\n",
    "    }\n",
    "    \n",
    "    return config\n",
    "\n",
    "def load_graphs_from_df(df, graph_dir, label_col='label'):\n",
    "    \"\"\"\n",
    "    從 DataFrame 載入圖資料。\n",
    "    新增了 label_col 參數來決定使用哪一欄作為標籤。\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        file_name = row['file_name']\n",
    "        prefix = file_name[:2]\n",
    "        label = row[label_col]\n",
    "        graph_path = Path(graph_dir) / prefix / f\"{file_name}.gpickle\"\n",
    "\n",
    "        if not graph_path.exists():\n",
    "            continue\n",
    "            \n",
    "        with open(graph_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        node_embeddings = data['node_embeddings']\n",
    "        if not node_embeddings:\n",
    "            continue\n",
    "        \n",
    "        embeddings = [list(emb) for emb in node_embeddings.values()]\n",
    "        x = torch.tensor(embeddings, dtype=torch.float)\n",
    "        \n",
    "        num_nodes = len(embeddings)\n",
    "        edge_list = []\n",
    "        for i in range(num_nodes - 1):\n",
    "            edge_list.extend([[i, i+1], [i+1, i]])\n",
    "        \n",
    "        if edge_list:\n",
    "            edge_index = torch.tensor(edge_list, dtype=torch.long).t()\n",
    "        else:\n",
    "            edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
    "        \n",
    "        graph_data = Data(x=x, edge_index=edge_index)\n",
    "        graphs.append(graph_data)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return graphs, labels\n",
    "\n",
    "def load_cross_arch_data_with_family_label(csv_path, graph_dir, source_cpus, target_cpus, cache_file, val_size=0.2, random_state=42, force_reload=False):\n",
    "    \"\"\"\n",
    "    載入跨架構的圖資料，並以 'family' 作為標籤。\n",
    "    \"\"\"\n",
    "    if force_reload and os.path.exists(cache_file):\n",
    "        os.remove(cache_file)\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Loading cached data from: {cache_file}\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            cached_data = pickle.load(f)\n",
    "        return (cached_data['train_graphs'], \n",
    "                cached_data['val_graphs'],\n",
    "                cached_data['test_graphs'], \n",
    "                cached_data['label_encoder'], \n",
    "                cached_data['num_classes'])\n",
    "    \n",
    "    print(\"Loading CSV data...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    train_df = df[df['CPU'].isin(source_cpus)]\n",
    "    test_df = df[df['CPU'].isin(target_cpus)]\n",
    "    \n",
    "    print(f\"Training data: {len(train_df)} samples (architectures: {source_cpus})\")\n",
    "    print(f\"Test data: {len(test_df)} samples (architectures: {target_cpus})\")\n",
    "    \n",
    "    train_graphs, train_labels = load_graphs_from_df(train_df, graph_dir, label_col='family')\n",
    "    test_graphs, test_labels = load_graphs_from_df(test_df, graph_dir, label_col='family')\n",
    "    \n",
    "    train_graphs, val_graphs, train_labels, val_labels = train_test_split(\n",
    "        train_graphs, train_labels, test_size=val_size, \n",
    "        stratify=train_labels, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    all_labels = train_labels + val_labels + test_labels\n",
    "    label_encoder.fit(all_labels)\n",
    "    \n",
    "    encoded_train_labels = label_encoder.transform(train_labels)\n",
    "    encoded_val_labels = label_encoder.transform(val_labels)\n",
    "    encoded_test_labels = label_encoder.transform(test_labels)\n",
    "    \n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    \n",
    "    for i, data in enumerate(train_graphs):\n",
    "        data.y = torch.tensor(encoded_train_labels[i], dtype=torch.long)\n",
    "        \n",
    "    for i, data in enumerate(val_graphs):\n",
    "        data.y = torch.tensor(encoded_val_labels[i], dtype=torch.long)\n",
    "        \n",
    "    for i, data in enumerate(test_graphs):\n",
    "        data.y = torch.tensor(encoded_test_labels[i], dtype=torch.long)\n",
    "\n",
    "    os.makedirs(os.path.dirname(cache_file), exist_ok=True)\n",
    "    cache_data = {\n",
    "        'train_graphs': train_graphs,\n",
    "        'val_graphs': val_graphs,\n",
    "        'test_graphs': test_graphs,\n",
    "        'label_encoder': label_encoder,\n",
    "        'num_classes': num_classes\n",
    "    }\n",
    "    \n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(cache_data, f)\n",
    "    \n",
    "    print(f\"Data has been cached to: {cache_file}\")\n",
    "    return train_graphs, val_graphs, test_graphs, label_encoder, num_classes\n",
    "\n",
    "def analyze_dataset_by_family(graphs, label_encoder):\n",
    "    \"\"\"\n",
    "    Analyzes the node distribution for each family and returns a dictionary of stats.\n",
    "    \"\"\"\n",
    "    family_stats = {}\n",
    "    if not graphs:\n",
    "        return family_stats\n",
    "\n",
    "    # Group graphs by family\n",
    "    family_groups = defaultdict(list)\n",
    "    for g in graphs:\n",
    "        family = label_encoder.inverse_transform([int(g.y)])[0]\n",
    "        family_groups[family].append(g.num_nodes)\n",
    "\n",
    "    for family, node_counts in family_groups.items():\n",
    "        family_stats[family] = {\n",
    "            \"Total Graphs\": len(node_counts),\n",
    "            \"Avg Nodes\": np.mean(node_counts),\n",
    "            \"Median Nodes\": np.median(node_counts),\n",
    "            \"Max Nodes\": np.max(node_counts),\n",
    "            \"Min Nodes\": np.min(node_counts)\n",
    "        }\n",
    "    return family_stats\n",
    "\n",
    "def compare_datasets(train_graphs, val_graphs, test_graphs, label_encoder):\n",
    "    \"\"\"\n",
    "    Compares key statistics of training, validation, and test datasets.\n",
    "    \"\"\"\n",
    "    # First, print the overall comparison report\n",
    "    train_stats = analyze_dataset_by_family(train_graphs, label_encoder)\n",
    "    test_stats = analyze_dataset_by_family(test_graphs, label_encoder)\n",
    "\n",
    "    all_families = sorted(list(set(train_stats.keys()) | set(test_stats.keys())))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Family-wise Node Count Comparison\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for family in all_families:\n",
    "        train_data = train_stats.get(family, {\"Total Graphs\": 0, \"Avg Nodes\": 0, \"Median Nodes\": 0, \"Max Nodes\": 0, \"Min Nodes\": 0})\n",
    "        test_data = test_stats.get(family, {\"Total Graphs\": 0, \"Avg Nodes\": 0, \"Median Nodes\": 0, \"Max Nodes\": 0, \"Min Nodes\": 0})\n",
    "        \n",
    "        print(f\"\\n--- Family: {family} ---\")\n",
    "        \n",
    "        comparison_df = pd.DataFrame({\n",
    "            \"Metric\": [\"Total Graphs\", \"Avg Nodes\", \"Median Nodes\", \"Max Nodes\", \"Min Nodes\"],\n",
    "            \"Training Set\": [train_data[\"Total Graphs\"], f\"{train_data['Avg Nodes']:.2f}\", train_data[\"Median Nodes\"], train_data[\"Max Nodes\"], train_data[\"Min Nodes\"]],\n",
    "            \"Test Set\": [test_data[\"Total Graphs\"], f\"{test_data['Avg Nodes']:.2f}\", test_data[\"Median Nodes\"], test_data[\"Max Nodes\"], test_data[\"Min Nodes\"]]\n",
    "        })\n",
    "        print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    gnn_config = get_gnn_config()\n",
    "    \n",
    "    train_graphs, val_graphs, test_graphs, label_encoder, num_classes = load_cross_arch_data_with_family_label(\n",
    "        csv_path=gnn_config[\"csv_path\"],\n",
    "        graph_dir=gnn_config[\"graph_dir\"],\n",
    "        source_cpus=gnn_config[\"source_cpus\"],\n",
    "        target_cpus=gnn_config[\"target_cpus\"],\n",
    "        cache_file=gnn_config[\"cache_file\"],\n",
    "        force_reload=False\n",
    "    )\n",
    "\n",
    "    compare_datasets(train_graphs, val_graphs, test_graphs, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775651c5",
   "metadata": {},
   "source": [
    "Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac1b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "path = \"/home/tommy/Project/PcodeBERT/outputs/alignment_sentences/train_x86_64_arm_32_embeddings.pickle\"\n",
    "\n",
    "with open(path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === 定義 similarity 函式 ===\n",
    "def similarity_score(x1, x2):\n",
    "    dist = torch.norm(x1 - x2, dim=1)  \n",
    "    return 1 / (1 + dist)             \n",
    "\n",
    "# === 批次計算平均 similarity ===\n",
    "scores = []\n",
    "for v1, v2, _ in tqdm(data, desc=\"計算中\"):\n",
    "    t1 = torch.tensor(v1, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    t2 = torch.tensor(v2, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    score = similarity_score(t1, t2)\n",
    "    scores.append(score.item())\n",
    "\n",
    "avg_score = np.mean(scores)\n",
    "print(f\"平均 similarity score: {avg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16541b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_path = \"/home/tommy/Project/PcodeBERT/outputs/alignment_sentences/train_x86_64_arm_32_embeddings.pickle\"\n",
    "output_path = \"/home/tommy/Project/PcodeBERT/outputs/alignment_sentences/train_x86_64_arm_32_balanced_embeddings.pickle\"\n",
    "\n",
    "with open(input_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === similarity 函式 ===\n",
    "def similarity_score(x1, x2):\n",
    "    dist = torch.norm(x1 - x2, dim=1)  # L2 distance\n",
    "    return 1 / (1 + dist)\n",
    "\n",
    "# === 計算所有 similarity score ===\n",
    "scored_data = []\n",
    "for v1, v2, label in tqdm(data, desc=\"計算相似度\"):\n",
    "    t1 = torch.tensor(v1, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    t2 = torch.tensor(v2, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    score = similarity_score(t1, t2).item()\n",
    "    scored_data.append((v1, v2, label, score))\n",
    "\n",
    "# === 依照相似度排序 ===\n",
    "scored_data.sort(key=lambda x: x[3], reverse=True)\n",
    " \n",
    "\n",
    "# === 分成兩半 ===\n",
    "half = len(scored_data) // 2\n",
    "positive_samples = [(v1, v2, 1) for v1, v2, _, _ in scored_data[:half]]\n",
    "negative_candidates = [ (v1, v2) for v1, v2, _, _ in scored_data[half:] ]\n",
    "\n",
    "print(f\"[INFO] 正樣本數: {len(positive_samples)}\")\n",
    "print(f\"[INFO] 負樣本候選數: {len(negative_candidates)}\")\n",
    "\n",
    "# === 打亂後配對建立負樣本 ===\n",
    "all_vec1 = [v1 for v1, _ in negative_candidates]\n",
    "all_vec2 = [v2 for _, v2 in negative_candidates]\n",
    "random.shuffle(all_vec2)\n",
    "\n",
    "negative_samples = [(v1, v2, 0) for v1, v2 in zip(all_vec1, all_vec2)]\n",
    "\n",
    "#print sample of negative samples\n",
    "print(\"\\n[INFO] 負樣本範例:\")\n",
    "for v1, v2, _ in negative_samples[:5]:\n",
    "    print(f\"  - {v1} <-> {v2}\")\n",
    "\n",
    "# === 合併正負樣本 ===\n",
    "final_dataset = positive_samples + negative_samples\n",
    "random.shuffle(final_dataset)\n",
    "\n",
    "# === 儲存 ===\n",
    "with open(output_path, \"wb\") as f:\n",
    "    pickle.dump(final_dataset, f)\n",
    "\n",
    "print(f\"[INFO] 原始樣本數: {len(data)}\")\n",
    "print(f\"[INFO] 新資料集樣本數: {len(final_dataset)}\")\n",
    "print(f\"[INFO] 已儲存至: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b39c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check label 1 and label 0 simiarity score\n",
    "import pickle \n",
    "\n",
    "with open(\"/home/tommy/Project/PcodeBERT/outputs/alignment_sentences/train_x86_64_arm_32_balanced_embeddings.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "#check label distribution\n",
    "from collections import Counter\n",
    "labels = [label for _, _, label in data]\n",
    "label_counts = Counter(labels)\n",
    "print(\"Label distribution:\", label_counts)\n",
    "\n",
    "for v1, v2, label in data:\n",
    "    if label == 1:\n",
    "        t1 = torch.tensor(v1, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        t2 = torch.tensor(v2, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        pos_score = similarity_score(t1, t2)\n",
    "        print(f\"Positive pair similarity score: {pos_score.item():.4f}\")\n",
    "        break\n",
    "for v1, v2, label in data:\n",
    "    if label == 0:\n",
    "        t1 = torch.tensor(v1, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        t2 = torch.tensor(v2, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        neg_score = similarity_score(t1, t2)\n",
    "        print(f\"Negative pair similarity score: {neg_score.item():.4f}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7618123a",
   "metadata": {},
   "source": [
    "Check performance adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a67ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AdapterMapper(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.mapper = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim), \n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        self.use_residual = (input_dim == output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_residual:\n",
    "            return x + self.mapper(x)\n",
    "        else:\n",
    "            return self.mapper(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fca835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaForMaskedLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdapterMapper(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.mapper = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim), \n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "        # self.mapper = nn.Sequential(\n",
    "        # nn.Linear(input_dim, hidden_dim),\n",
    "        # nn.GELU(), \n",
    "        # nn.LayerNorm(hidden_dim),\n",
    "        # nn.Linear(hidden_dim, hidden_dim), \n",
    "        # nn.GELU(),\n",
    "        # nn.LayerNorm(hidden_dim),\n",
    "        # nn.Linear(hidden_dim, output_dim)\n",
    "        # )   \n",
    "        self.use_residual = (input_dim == output_dim)\n",
    "    def forward(self, x):\n",
    "        if self.use_residual:\n",
    "            return x + self.mapper(x)\n",
    "        else:\n",
    "            return self.mapper(x)\n",
    "\n",
    "# === 定義 similarity 函式 ===\n",
    "# def similarity_score(x1, x2):\n",
    "#     dist = torch.norm(x1 - x2, dim=1)  \n",
    "#     return 1 / (1 + dist)\n",
    "\n",
    "# def similarity_score(x1, x2):\n",
    "#     # F.cosine_similarity 輸出的範圍是 -1 到 1\n",
    "#     # 這裡假設 x1 和 x2 都是 (batch_size, dim)\n",
    "#     sim = F.cosine_similarity(x1, x2, dim=1)\n",
    "    \n",
    "#     # 您可以選擇是否要將其轉換到 0 到 1 之間\n",
    "#     # return (sim + 1) / 2 \n",
    "#     return sim\n",
    "\n",
    "def similarity_score(x1, x2):\n",
    "    # 1. 計算兩個向量的夾角 (的餘弦值)\n",
    "    # 範例：x1=[2, 0], x2=[3, 0] -> 夾角 0 度, sim = 1\n",
    "    # 範例：x1=[2, 0], x2=[0, 2] -> 夾角 90 度, sim = 0\n",
    "    # 範例：x1=[2, 0], x2=[-2, 0] -> 夾角 180 度, sim = -1\n",
    "    sim = F.cosine_similarity(x1, x2, dim=1)\n",
    "    \n",
    "    # 2. 直接返回該值 (範圍 -1 到 1)\n",
    "    return sim\n",
    "\n",
    "# --- 1. 設定 ---\n",
    "FILE_PATH = \"/home/tommy/Project/PcodeBERT/outputs/alignment_sentences/train_x86_64_arm_32_functions_deduped.pickle\"\n",
    "BERT_PATH = '/home/tommy/Project/PcodeBERT/outputs/model_epoch_50'\n",
    "ADAPTER_PATH = \"/home/tommy/Project/PcodeBERT/outputs/adapter/adapter_model_100_cosine.pt\"\n",
    "\n",
    "NUM_SAMPLES = 1000\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 2. 載入模型 ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_PATH)\n",
    "roberta = RobertaForMaskedLM.from_pretrained(BERT_PATH).to(DEVICE)\n",
    "roberta.eval()\n",
    "\n",
    "hidden_size = roberta.config.hidden_size\n",
    "print(f\"RoBERTa hidden size: {hidden_size}\")\n",
    "\n",
    "adapter = AdapterMapper(input_dim=hidden_size, output_dim=hidden_size).to(DEVICE)\n",
    "state_dict = torch.load(ADAPTER_PATH, map_location=DEVICE)\n",
    "adapter.load_state_dict(state_dict)\n",
    "adapter.eval()\n",
    "print(\"Adapter state_dict 載入成功。\")\n",
    "\n",
    "# --- 3. 嵌入函數 ---\n",
    "def text_to_embedding(texts, model, tokenizer, device):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.roberta(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "# --- 4. 載入資料 ---\n",
    "with open(FILE_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data = data[:NUM_SAMPLES]\n",
    "texts1 = [d[0] for d in data]\n",
    "texts2 = [d[1] for d in data]\n",
    "\n",
    "# --- 5. 批次處理以獲取嵌入 ---\n",
    "v1_roberta_list, v2_roberta_list = [], []\n",
    "v1_adapter_list, v2_adapter_list = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(texts1), BATCH_SIZE):\n",
    "        print(f\"Processing batch {i//BATCH_SIZE + 1}...\")\n",
    "        batch_texts1 = texts1[i:i+BATCH_SIZE]\n",
    "        batch_texts2 = texts2[i:i+BATCH_SIZE]\n",
    "\n",
    "        r_v1 = text_to_embedding(batch_texts1, roberta, tokenizer, DEVICE)\n",
    "        r_v2 = text_to_embedding(batch_texts2, roberta, tokenizer, DEVICE)\n",
    "        \n",
    "        a_v1 = adapter(r_v1)\n",
    "        a_v2 = adapter(r_v2)\n",
    "\n",
    "        v1_roberta_list.append(r_v1.cpu())\n",
    "        v2_roberta_list.append(r_v2.cpu())\n",
    "        v1_adapter_list.append(a_v1.cpu())\n",
    "        v2_adapter_list.append(a_v2.cpu())\n",
    "\n",
    "v1_roberta = torch.cat(v1_roberta_list, dim=0)\n",
    "v2_roberta = torch.cat(v2_roberta_list, dim=0)\n",
    "v1_adapter = torch.cat(v1_adapter_list, dim=0)\n",
    "v2_adapter = torch.cat(v2_adapter_list, dim=0)\n",
    "\n",
    "# --- 6. 計算相似度與差異 ---\n",
    "sim_roberta = similarity_score(v1_roberta, v2_roberta)\n",
    "sim_adapter = similarity_score(v1_adapter, v2_adapter)\n",
    "differences = sim_adapter - sim_roberta\n",
    "\n",
    "# --- 7. 顯示結果 ---\n",
    "print(\"\\n--- 比較結果 (前 5 筆) ---\")\n",
    "for i in range(min(5, NUM_SAMPLES)):\n",
    "    print(f\"樣本 {i}: RoBERTa Sim={sim_roberta[i]:.4f}, Adapter Sim={sim_adapter[i]:.4f}, 差距={differences[i]:+.4f}\")\n",
    "\n",
    "print(\"\\n--- 總體平均 ---\")\n",
    "print(f\"平均 RoBERTa 相似度: {sim_roberta.mean().item():.4f}\")\n",
    "print(f\"平均 Adapter 相似度: {sim_adapter.mean().item():.4f}\")\n",
    "print(f\"平均 相似度差距 (Adapter - RoBERTa): {differences.mean().item():+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93ed6a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommy/miniconda3/envs/PcodeBERT/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from adapters import AutoAdapterModel\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "class AdapterEmbeddingModel(nn.Module):\n",
    "    def __init__(self, model_name, adapter_config, adapter_name, \n",
    "                 input_dim=256, output_dim=256, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.adapter_name = adapter_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        self.model = AutoAdapterModel.from_pretrained(model_name)\n",
    "        self.model.add_adapter(adapter_name, config=adapter_config)\n",
    "        self.model.train_adapter(adapter_name)\n",
    "        self.model.set_active_adapters(adapter_name)\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'adapter' not in name.lower():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        total = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"\\nAdapter Model: {total:,} total, {trainable:,} trainable ({trainable/total*100:.1f}%)\\n\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                           output_hidden_states=True)\n",
    "        return outputs.hidden_states[-1][:, 0, :]\n",
    "    \n",
    "    def save_adapter(self, save_path):\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        self.model.save_adapter(save_path, self.adapter_name)\n",
    "    \n",
    "    def load_adapter(self, load_path):\n",
    "        self.model.load_adapter(load_path, load_as=self.adapter_name)\n",
    "        self.model.set_active_adapters(self.adapter_name)\n",
    "\"\"\"\n",
    "Adapter 訓練配置檔案\n",
    "使用 adapter-transformers 套件進行 Adapter 訓練\n",
    "\"\"\"\n",
    "\n",
    "def get_adapter_config():\n",
    "    \"\"\"\n",
    "    返回 Adapter 訓練的配置參數\n",
    "    \n",
    "    Returns:\n",
    "        dict: 包含所有訓練參數的配置字典\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model_name\": \"/home/tommy/Project/PcodeBERT/outputs/models/RoBERTa/model_epoch_100\",\n",
    "        \"adapter_name\": \"pcode_adapter\",  \n",
    "        \n",
    "        \"adapter_config\": \"pfeiffer\",\n",
    "        \"reduction_factor\": 32,  \n",
    "        \"non_linearity\": \"gelu\",  \n",
    "        \"leave_out\": [0, 1, 2, 3, 4], \n",
    "        \"input_dim\": 256,  \n",
    "        \"output_dim\": 256,\n",
    "        \"hidden_dim\": 128, \n",
    "        \"use_projection\": False, \n",
    "        \n",
    "        \"data_path\": \"/home/tommy/Project/PcodeBERT/outputs/data/Adapters/train_x86_64_arm_32_functions_deduped.pickle\",\n",
    "        \"val_data_path\": None,  \n",
    "        \"val_split\": 0.2,  \n",
    "        \n",
    "        \"batch_size\": 128,\n",
    "        \"learning_rate\": 1e-4, \n",
    "        \"epochs\": 5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \n",
    "        \"loss_functions\": [\"mse\"],\n",
    "        \"triplet_margin\": 1.0,\n",
    "        \"triplet_p\": 2,\n",
    "        \n",
    "        \"scheduler_type\": \"cosine\",  \n",
    "        \"scheduler_patience\": 10,  \n",
    "        \"scheduler_factor\": 0.5,   \n",
    "        \n",
    "        \"early_stop_patience\": 10,\n",
    "        \n",
    "        \"device\": \"cuda\",\n",
    "        \"save_dir\": \"/home/tommy/Project/PcodeBERT/outputs/adapter\",\n",
    "        \"save_model_name\": \"adapter_roberta\",  \n",
    "        \n",
    "        \"max_length\": 512, \n",
    "        \"seed\": 42,\n",
    "        \"log_interval\": 10,  \n",
    "    }\n",
    "\n",
    "\n",
    "def get_inference_config():\n",
    "    \"\"\"\n",
    "    返回推理/應用 Adapter 的配置參數\n",
    "    \n",
    "    Returns:\n",
    "        dict: 推理配置字典\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"model_path\": \"/home/tommy/Project/PcodeBERT/outputs/models/RoBERTa/model_epoch_100\",\n",
    "        \"adapter_path\": \"/home/tommy/Project/PcodeBERT/outputs/adapter/adapter_roberta_mse\",\n",
    "        \"adapter_name\": \"pcode_adapter\",\n",
    "        \"input_path\": \"/home/tommy/Project/PcodeBERT/outputs/data/GNN/gpickle_merged_adjusted_filtered\",\n",
    "        \"output_path\": \"/home/tommy/Project/PcodeBERT/outputs/data/GNN/gpickle_merged_adjusted_filtered_adapter\",\n",
    "        \"csv_path\": \"/home/tommy/Project/PcodeBERT/dataset/csv/merged_adjusted_filtered.csv\",\n",
    "        \"target_cpus\": [\"x86_64\", \"ARM\"],\n",
    "        \"batch_size\": 64,\n",
    "        \"device\": \"cuda\",\n",
    "        \"max_length\": 512,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0fbd613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at /home/tommy/Project/PcodeBERT/outputs/models/RoBERTa/model_epoch_100 and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Adapter Model: 11,330,387 total, 4,360 trainable (0.0%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting existing adapter 'pcode_adapter'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings...\n",
      "\n",
      "--- 結果 (前 5 筆) ---\n",
      "樣本 0: Adapter Sim=0.9943\n",
      "樣本 1: Adapter Sim=1.0000\n",
      "樣本 2: Adapter Sim=0.9668\n",
      "樣本 3: Adapter Sim=0.9891\n",
      "樣本 4: Adapter Sim=0.9958\n",
      "\n",
      "--- 總體統計 ---\n",
      "平均相似度: 0.9875\n",
      "中位數: 0.9895\n",
      "標準差: 0.0096\n",
      "最小值: 0.9136\n",
      "最大值: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/tommy/Project/PcodeBERT/src')\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from adapters import AdapterConfig\n",
    "\n",
    "def similarity_score(x1, x2):\n",
    "    return F.cosine_similarity(x1, x2, dim=1)\n",
    "\n",
    "# --- 設定 ---\n",
    "FILE_PATH = \"/home/tommy/Project/PcodeBERT/outputs/alignment_sentences/train_x86_64_arm_32_functions_deduped.pickle\"\n",
    "ADAPTER_PATH = \"/home/tommy/Project/PcodeBERT/outputs/adapter/adapter_roberta_mse\"\n",
    "NUM_SAMPLES = 1000\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 載入配置和模型 ---\n",
    "config = get_adapter_config()\n",
    "\n",
    "adapter_config = AdapterConfig.load(\n",
    "    config[\"adapter_config\"],\n",
    "    reduction_factor=config[\"reduction_factor\"],\n",
    "    non_linearity=config[\"non_linearity\"],\n",
    "    leave_out=config[\"leave_out\"]\n",
    ")\n",
    "\n",
    "model = AdapterEmbeddingModel(\n",
    "    model_name=config[\"model_name\"],\n",
    "    adapter_config=adapter_config,\n",
    "    adapter_name=config[\"adapter_name\"],\n",
    "    input_dim=config[\"input_dim\"],\n",
    "    output_dim=config[\"output_dim\"],\n",
    "    hidden_dim=config[\"hidden_dim\"]\n",
    ").to(DEVICE)\n",
    "\n",
    "model.load_adapter(ADAPTER_PATH)\n",
    "model.eval()\n",
    "\n",
    "# --- 嵌入函數 ---\n",
    "def get_embeddings(texts, model, device, batch_size=32):\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = model.tokenizer(batch_texts, padding=True, truncation=True, \n",
    "                                max_length=512, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeddings = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "    \n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "# --- 載入資料 ---\n",
    "with open(FILE_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data = data[:NUM_SAMPLES]\n",
    "texts1 = [d[0] for d in data]\n",
    "texts2 = [d[1] for d in data]\n",
    "\n",
    "# --- 獲取嵌入 ---\n",
    "print(\"Computing embeddings...\")\n",
    "v1_adapter = get_embeddings(texts1, model, DEVICE, BATCH_SIZE)\n",
    "v2_adapter = get_embeddings(texts2, model, DEVICE, BATCH_SIZE)\n",
    "\n",
    "# --- 計算相似度 ---\n",
    "sim_adapter = similarity_score(v1_adapter, v2_adapter)\n",
    "\n",
    "# --- 顯示結果 ---\n",
    "print(\"\\n--- 結果 (前 5 筆) ---\")\n",
    "for i in range(min(5, NUM_SAMPLES)):\n",
    "    print(f\"樣本 {i}: Adapter Sim={sim_adapter[i]:.4f}\")\n",
    "\n",
    "print(\"\\n--- 總體統計 ---\")\n",
    "print(f\"平均相似度: {sim_adapter.mean().item():.4f}\")\n",
    "print(f\"中位數: {sim_adapter.median().item():.4f}\")\n",
    "print(f\"標準差: {sim_adapter.std().item():.4f}\")\n",
    "print(f\"最小值: {sim_adapter.min().item():.4f}\")\n",
    "print(f\"最大值: {sim_adapter.max().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038be4c5",
   "metadata": {},
   "source": [
    "Check training data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7a3527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "file = \"/home/tommy/Project/PcodeBERT/outputs/alignment_sentences/train_x86_64_arm_32_balanced_embeddings.pickle\"\n",
    "\n",
    "with open(file, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "#check pickle type\n",
    "\n",
    "print(f\"Data type: {type(data)}\")\n",
    "print(f\"First item type: {type(data[0])}\")\n",
    "print(f\"First item content: {data[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05ce79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'list'>\n",
      "First item type: <class 'list'>\n",
      "First item content: ['LOAD', 'UNIQUE', 'CONST', 'REG', 'INT_ZEXT', 'REG', 'UNIQUE', 'INT_ADD', 'UNIQUE', 'REG', 'CONST', 'LOAD', 'UNIQUE', 'CONST', 'UNIQUE', 'CAST', 'UNIQUE', 'UNIQUE', 'COPY', 'STACK', 'UNIQUE', 'LOAD', 'UNIQUE', 'CONST', 'UNIQUE', 'PTRSUB', 'UNIQUE', 'CONST', 'CONST', 'PTRADD', 'UNIQUE', 'UNIQUE', 'REG', 'CONST', 'INT_LESSEQUAL', 'UNIQUE', 'REG', 'UNIQUE', 'CBRANCH', 'MEM', 'UNIQUE', 'CAST', 'UNIQUE', 'UNIQUE', 'INT_EQUAL', 'REG', 'UNIQUE', 'CONST', 'LOAD', 'UNIQUE', 'CONST', 'UNIQUE', 'PTRADD', 'UNIQUE', 'REG', 'CONST', 'CONST', 'CAST', 'UNIQUE', 'UNIQUE', 'CBRANCH', 'MEM', 'REG', 'INT_NOTEQUAL', 'REG', 'UNIQUE', 'CONST', 'CBRANCH', 'MEM', 'REG', 'LOAD', 'UNIQUE', 'CONST', 'UNIQUE', 'INT_AND', 'UNIQUE', 'UNIQUE', 'CONST', 'STORE', 'CONST', 'UNIQUE', 'UNIQUE', 'INDIRECT', 'MEM', 'MEM', 'CONST', 'PTRADD', 'UNIQUE', 'REG', 'CONST', 'CONST', 'STORE', 'CONST', 'UNIQUE', 'CONST', 'INDIRECT', 'MEM', 'MEM', 'CONST', 'PTRADD', 'UNIQUE', 'REG', 'CONST', 'CONST', 'PTRADD', 'UNIQUE', 'REG', 'CONST', 'CONST', 'STORE', 'CONST', 'UNIQUE', 'CONST', 'MULTIEQUAL', 'REG', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'REG', 'UNIQUE', 'MULTIEQUAL', 'REG', 'REG', 'REG', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'REG', 'REG', 'REG', 'MULTIEQUAL', 'MEM', 'MEM', 'MEM', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'MEM', 'MEM', 'MEM', 'MULTIEQUAL', 'STACK', 'STACK', 'STACK', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'STACK', 'STACK', 'STACK', 'MULTIEQUAL', 'STACK', 'STACK', 'STACK', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'STACK', 'STACK', 'STACK', 'MULTIEQUAL', 'STACK', 'STACK', 'STACK', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'STACK', 'STACK', 'STACK', 'INT_ADD', 'UNIQUE', 'REG', 'CONST', 'LOAD', 'UNIQUE', 'CONST', 'UNIQUE', 'INT_NOTEQUAL', 'REG', 'STACK', 'UNIQUE', 'CAST', 'UNIQUE', 'UNIQUE', 'COPY', 'REG', 'REG', 'CBRANCH', 'MEM', 'REG', 'RETURN', 'CONST', 'REG', 'COPY', 'MEM', 'MEM', 'INT_MULT', 'UNIQUE', 'UNIQUE', 'CONST', 'INT_NOTEQUAL', 'REG', 'UNIQUE', 'CONST', 'INT_SUB', 'REG', 'UNIQUE', 'CONST', 'CBRANCH', 'MEM', 'REG', 'COPY', 'STACK', 'UNIQUE', 'PTRSUB', 'UNIQUE', 'REG', 'CONST', 'INT_AND', 'UNIQUE', 'REG', 'CONST', 'INT_RIGHT', 'REG', 'REG', 'UNIQUE', 'SUBPIECE', 'UNIQUE', 'UNIQUE', 'CONST', 'INT_AND', 'REG', 'REG', 'CONST', 'SUBPIECE', 'REG', 'REG', 'CONST', 'INT_SUB', 'UNIQUE', 'UNIQUE', 'CONST', 'INT_NOTEQUAL', 'UNIQUE', 'UNIQUE', 'CONST', 'INT_SEXT', 'REG', 'UNIQUE', 'CAST', 'UNIQUE', 'REG', 'COPY', 'STACK', 'REG', 'INT_ADD', 'UNIQUE', 'UNIQUE', 'CONST', 'INT_MULT', 'UNIQUE', 'REG', 'CONST', 'INT_ADD', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'LOAD', 'UNIQUE', 'CONST', 'UNIQUE', 'CAST', 'UNIQUE', 'UNIQUE', 'INT_ADD', 'UNIQUE', 'UNIQUE', 'CONST', 'CBRANCH', 'MEM', 'UNIQUE', 'MULTIEQUAL', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'MULTIEQUAL', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'MULTIEQUAL', 'UNIQUE', 'REG', 'REG', 'MULTIEQUAL', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'MULTIEQUAL', 'UNIQUE', 'UNIQUE', 'REG', 'MULTIEQUAL', 'UNIQUE', 'STACK', 'STACK', 'MULTIEQUAL', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'COPY', 'UNIQUE', 'UNIQUE', 'COPY', 'UNIQUE', 'UNIQUE', 'COPY', 'UNIQUE', 'UNIQUE', 'INT_NOTEQUAL', 'REG', 'UNIQUE', 'CONST', 'CBRANCH', 'MEM', 'REG', 'INT_ADD', 'UNIQUE', 'UNIQUE', 'CONST', 'LOAD', 'UNIQUE', 'CONST', 'UNIQUE', 'CAST', 'UNIQUE', 'UNIQUE', 'INT_LEFT', 'REG', 'CONST', 'REG', 'SUBPIECE', 'REG', 'UNIQUE', 'CONST', 'INT_AND', 'UNIQUE', 'REG', 'UNIQUE', 'INT_NOTEQUAL', 'REG', 'UNIQUE', 'CONST', 'CBRANCH', 'MEM', 'REG', 'INT_ADD', 'UNIQUE', 'UNIQUE', 'CONST', 'LOAD', 'UNIQUE', 'CONST', 'UNIQUE', 'MULTIEQUAL', 'REG', 'REG', 'REG', 'MULTIEQUAL', 'MEM', 'MEM', 'MEM', 'MULTIEQUAL', 'STACK', 'STACK', 'STACK', 'MULTIEQUAL', 'STACK', 'STACK', 'STACK', 'MULTIEQUAL', 'STACK', 'UNIQUE', 'STACK', 'CAST', 'UNIQUE', 'UNIQUE', 'INT_AND', 'UNIQUE', 'REG', 'UNIQUE', 'INT_NOTEQUAL', 'REG', 'UNIQUE', 'CONST', 'CBRANCH', 'MEM', 'REG', 'LOAD', 'UNIQUE', 'CONST', 'UNIQUE', 'MULTIEQUAL', 'REG', 'REG', 'REG', 'MULTIEQUAL', 'MEM', 'MEM', 'MEM', 'MULTIEQUAL', 'STACK', 'STACK', 'STACK', 'MULTIEQUAL', 'STACK', 'STACK', 'STACK', 'MULTIEQUAL', 'STACK', 'STACK', 'STACK', 'CAST', 'UNIQUE', 'UNIQUE', 'INT_EQUAL', 'REG', 'UNIQUE', 'CONST', 'CBRANCH', 'MEM', 'REG', 'LOAD', 'UNIQUE', 'CONST', 'UNIQUE', 'INT_SEXT', 'REG', 'UNIQUE', 'PTRADD', 'UNIQUE', 'UNIQUE', 'CONST', 'CONST', 'SUBPIECE', 'UNIQUE', 'UNIQUE', 'CONST', 'STORE', 'CONST', 'UNIQUE', 'CONST', 'INDIRECT', 'MEM', 'MEM', 'CONST', 'INT_MULT', 'UNIQUE', 'REG', 'CONST', 'INT_ADD', 'UNIQUE', 'UNIQUE', 'CONST', 'PTRADD', 'UNIQUE', 'UNIQUE', 'UNIQUE', 'CONST', 'PTRADD', 'UNIQUE', 'UNIQUE', 'CONST', 'CONST', 'PTRADD', 'UNIQUE', 'UNIQUE', 'CONST', 'CONST', 'STORE', 'CONST', 'UNIQUE', 'CONST', 'LOAD', 'UNIQUE', 'CONST', 'UNIQUE', 'INT_ADD', 'UNIQUE', 'UNIQUE', 'CONST', 'STORE', 'CONST', 'UNIQUE', 'UNIQUE', 'LOAD', 'UNIQUE', 'CONST', 'UNIQUE', 'INT_EQUAL', 'UNIQUE', 'UNIQUE', 'CONST']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "file = \"/home/tommy/Project/PcodeBERT/outputs/preprocessed/pcode_corpus_x86_64_new_data.pkl\"\n",
    "\n",
    "with open(file, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(f\"Data type: {type(data)}\")\n",
    "print(f\"First item type: {type(data[0])}\")\n",
    "print(f\"First item content: {data[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ef67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PcodeBERT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
